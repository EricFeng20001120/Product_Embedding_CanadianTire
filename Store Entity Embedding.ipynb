{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Embedding\n",
    "Entity embedding is a technique used in machine learning to represent categorical variables as continuous vectors, or embeddings. These embeddings can then be used as input to neural networks, allowing the model to better handle categorical variables and make more accurate predictions. Entity embeddings are typically learned by training a neural network on a large dataset of examples, where the categorical variable is one of the input features. The embeddings are learned as part of the model's parameters, and can be used for a variety of tasks, such as natural language processing and recommendation systems. Entity embedding allows models to understand the relationships and similarities between categorical variables, which can be useful for tasks such as product recommendations, where it is important to understand the similarities between different products."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference: https://grabngoinfo.com/categorical-entity-embedding-using-python-tensorflow-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import portion of a package\n",
    "import matplotlib.pyplot as plt  # Most common visualization package that a lot of others are based on\n",
    "\n",
    "# Import full packages under custom name\n",
    "import numpy as np  # Common package for numerical methods\n",
    "import pandas as pd  # Common package for data storeage/manipulation\n",
    "import seaborn as sns  # Common package for statistical visualizations\n",
    "\n",
    "# Import portion of a package\n",
    "import scipy.stats as stats\n",
    "from sklearn.impute import SimpleImputer as Imputer  # Specific function from common machine learning package\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get weekly sales data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yr_num</th>\n",
       "      <th>wk_num</th>\n",
       "      <th>product_num</th>\n",
       "      <th>store_num</th>\n",
       "      <th>sales_qty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021</td>\n",
       "      <td>36</td>\n",
       "      <td>5010</td>\n",
       "      <td>241</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021</td>\n",
       "      <td>36</td>\n",
       "      <td>5044</td>\n",
       "      <td>181</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021</td>\n",
       "      <td>36</td>\n",
       "      <td>5044</td>\n",
       "      <td>233</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021</td>\n",
       "      <td>36</td>\n",
       "      <td>5044</td>\n",
       "      <td>241</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021</td>\n",
       "      <td>36</td>\n",
       "      <td>5044</td>\n",
       "      <td>466</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223378088</th>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>8997134</td>\n",
       "      <td>353</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223378089</th>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>8997134</td>\n",
       "      <td>448</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223378090</th>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>8997134</td>\n",
       "      <td>450</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223378091</th>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>8997134</td>\n",
       "      <td>486</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223378092</th>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>8997134</td>\n",
       "      <td>661</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>213318131 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           yr_num  wk_num  product_num  store_num  sales_qty\n",
       "0            2021      36         5010        241          3\n",
       "1            2021      36         5044        181          1\n",
       "2            2021      36         5044        233          4\n",
       "3            2021      36         5044        241          1\n",
       "4            2021      36         5044        466          1\n",
       "...           ...     ...          ...        ...        ...\n",
       "223378088    2022       9      8997134        353          3\n",
       "223378089    2022       9      8997134        448          1\n",
       "223378090    2022       9      8997134        450          5\n",
       "223378091    2022       9      8997134        486          4\n",
       "223378092    2022       9      8997134        661          1\n",
       "\n",
       "[213318131 rows x 5 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_path = \"clean_data\\cleaned_sales.csv\"\n",
    "df_sales_data = pd.read_csv(sales_path, index_col=0, header=0)\n",
    "df_sales_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sales quantity for each store given week and year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnesssary column\n",
    "df_sale_copy = df_sales_data.copy()\n",
    "df_sale_copy = df_sales_data.drop(['product_num'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sale_clean = df_sale_copy.groupby(['yr_num','wk_num','store_num']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we groupby and get sales quantity for each store per week per year\n",
    "df_sale_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sale_clean = df_sale_clean.reset_index()\n",
    "df_sale_clean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge store data and weekly sale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_store = \"clean_data/cleaned_store.csv\" #Eric\n",
    "# put it in data farme\n",
    "df_store = pd.read_csv(path_store, index_col=0, header=0)\n",
    "df_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge sales and store data on key \"store_num\"\n",
    "merged_data = pd.merge(df_sale_clean, df_store, on='store_num', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn binary data into numerical data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onsite propane include yes and no, turn to 0 and 1\n",
    "winterized canopy inlude yes, no, and not determined, turn to 0,1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['onsite_propane_txt'] = merged_data['onsite_propane_txt'].map({'Yes': 0, 'No': 1})\n",
    "merged_data['winterized_canopy_txt'] = merged_data['winterized_canopy_txt'].map({'Yes': 0, 'No': 1, 'Not Determined': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['winterized_canopy_txt']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale numerical data, so the model data will scale between 0 and 1, which make model less sensitive to large numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# select the numerical columns from the store data\n",
    "numerical_cols = ['latitude_qty', 'longitude_qty','retail_square_ft_qty','ins_garden_centre_sqr_ft_qty','number_of_service_bays_qty','checkouts_count']\n",
    "store_data_numerical = merged_data[numerical_cols]\n",
    "\n",
    "# initialize the scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# fit the scaler to the numerical columns\n",
    "scaler.fit(store_data_numerical)\n",
    "\n",
    "# transform the numerical columns\n",
    "store_data_numerical = scaler.transform(store_data_numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new DataFrame with the preprocessed numerical columns\n",
    "store_data_numerical = pd.DataFrame(store_data_numerical, columns=numerical_cols)\n",
    "\n",
    "merged_data.drop(columns=numerical_cols, inplace=True)\n",
    "store_data = pd.concat([merged_data, store_data_numerical], axis=1)\n",
    "store_data = store_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_data = store_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert categorical data into label data, so each unique value inside a categorical data will be replaced by int value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Create a sample DataFrame\n",
    "categorical_cols = ['store_nm', 'province_cd','store_size_cd','store_concept_type_nm','shopping_centre_nm']\n",
    "# Create a label encoder\n",
    "le = LabelEncoder()\n",
    "mappings = []\n",
    "# Fit and transform the column 'A'\n",
    "for i in categorical_cols:\n",
    "    original = store_data[i]\n",
    "    store_data[i] = le.fit_transform(store_data[i])\n",
    "    mappings.append(dict (zip(original, store_data[i])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save encodings if we want to reference them later\n",
    "store_nm_encoding = mappings[0]\n",
    "province_cd_encoding = mappings[1]\n",
    "store_size_cd_encoding = mappings[2]\n",
    "store_concept_type_nm_encoding = mappings[3]\n",
    "shopping_centre_nm_encoding = mappings[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = ['latitude_qty', 'longitude_qty','retail_square_ft_qty','ins_garden_centre_sqr_ft_qty','number_of_service_bays_qty','checkouts_count','wk_num','onsite_propane_txt','winterized_canopy_txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Features\n",
    "X = store_data.iloc[:, 1:].copy().drop('sales_qty', axis=1)\n",
    "\n",
    "# Target\n",
    "y = store_data['sales_qty']\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\n",
    "\n",
    "# Check the number of records in training and testing dataset.\n",
    "print(f'The training dataset has {X_train.shape[0]} records and {X_train.shape[1]} columns.')\n",
    "print(f'The testing dataset has {len(X_test)} records.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input list for the training data\n",
    "input_list_train = []\n",
    "\n",
    "# Input list for the testing data\n",
    "input_list_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 9) dtype=float32 (created by layer 'input_19')>"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Embedding, Dense\n",
    "\n",
    "# Input dimension of the numeric variables\n",
    "input_numeric = Input(shape=(len(numerical_cols),))\n",
    "\n",
    "# Output dimension of the numeric variables\n",
    "emb_numeric = input_numeric\n",
    "\n",
    "# Take a look at the output dimension\n",
    "emb_numeric"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create NN and create embedding layer for each categorical data, adding dense vector to learn additional relationship regarding to predicting weekly sale\n",
    "the model will contain embedding layer for each type of categorical column. and then the neural network will try to predict the weekly sale, which the embedding will adjust weight based on mean square error on predicting weekly sale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from keras.layers import Input, Embedding, Dense\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "input_layers = []\n",
    "output_layers = []\n",
    "\n",
    "\n",
    "for i in categorical_cols:\n",
    "    n_unique_day = store_data[i].nunique()\n",
    "    n_dim_day = int(sqrt(n_unique_day))\n",
    "    input_week = Input(shape=(1, ))\n",
    "    input_layers.append(input_week)\n",
    "    output_week = Embedding(input_dim=n_unique_day, \n",
    "                        output_dim=n_dim_day, name=str(i))(input_week)\n",
    "    output_week = tf.keras.layers.Reshape(target_shape=(n_dim_day, ))(output_week)\n",
    "    output_layers.append(output_week)\n",
    "\n",
    "# Input data dimensions\n",
    "input_layers.append(input_numeric)\n",
    "\n",
    "# Take a look at the data\n",
    "input_layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding dimensions\n",
    "emb_numeric = input_numeric\n",
    "\n",
    "output_layers.append(emb_numeric)\n",
    "\n",
    "# Take a look at the data\n",
    "output_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_20 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_21 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_22 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_23 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_24 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " store_nm (Embedding)           (None, 1, 22)        11132       ['input_20[0][0]']               \n",
      "                                                                                                  \n",
      " province_cd (Embedding)        (None, 1, 3)         36          ['input_21[0][0]']               \n",
      "                                                                                                  \n",
      " store_size_cd (Embedding)      (None, 1, 2)         12          ['input_22[0][0]']               \n",
      "                                                                                                  \n",
      " store_concept_type_nm (Embeddi  (None, 1, 2)        16          ['input_23[0][0]']               \n",
      " ng)                                                                                              \n",
      "                                                                                                  \n",
      " shopping_centre_nm (Embedding)  (None, 1, 9)        837         ['input_24[0][0]']               \n",
      "                                                                                                  \n",
      " reshape_15 (Reshape)           (None, 22)           0           ['store_nm[0][0]']               \n",
      "                                                                                                  \n",
      " reshape_16 (Reshape)           (None, 3)            0           ['province_cd[0][0]']            \n",
      "                                                                                                  \n",
      " reshape_17 (Reshape)           (None, 2)            0           ['store_size_cd[0][0]']          \n",
      "                                                                                                  \n",
      " reshape_18 (Reshape)           (None, 2)            0           ['store_concept_type_nm[0][0]']  \n",
      "                                                                                                  \n",
      " reshape_19 (Reshape)           (None, 9)            0           ['shopping_centre_nm[0][0]']     \n",
      "                                                                                                  \n",
      " input_19 (InputLayer)          [(None, 9)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 47)           0           ['reshape_15[0][0]',             \n",
      "                                                                  'reshape_16[0][0]',             \n",
      "                                                                  'reshape_17[0][0]',             \n",
      "                                                                  'reshape_18[0][0]',             \n",
      "                                                                  'reshape_19[0][0]',             \n",
      "                                                                  'input_19[0][0]']               \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 200)          9600        ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 200)          0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 100)          20100       ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 100)          0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 1)            101         ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 1)            0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 41,834\n",
      "Trainable params: 41,834\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:lr is deprecated in `optimizer_experimental.Optimizer`, please check the docstring for valid arguments.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.layers.Concatenate()(output_layers)\n",
    "# Add a few hidden layers\n",
    "model = Dense(200, kernel_initializer=\"uniform\")(model)\n",
    "model = tf.keras.layers.Activation('relu')(model)\n",
    "model = Dense(100, kernel_initializer=\"uniform\")(model)\n",
    "model = tf.keras.layers.Activation('relu')(model)\n",
    "# And finally our output layer\n",
    "model = Dense(1)(model)\n",
    "model = tf.keras.layers.Activation('sigmoid')(model)\n",
    "# Put it all together and compile the model\n",
    "model = Model(inputs=input_layers, outputs=model)\n",
    "model.summary()\n",
    "opt = tf.keras.optimizers.experimental.SGD(lr=0.05)\n",
    "model.compile(loss='mse', optimizer=opt, metrics=['mse'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layers\n",
    "categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "823/823 [==============================] - 2s 1ms/step - loss: 680163328.0000 - mse: 680163328.0000\n",
      "Epoch 2/10\n",
      "823/823 [==============================] - 1s 1ms/step - loss: 680163648.0000 - mse: 680163648.0000\n",
      "Epoch 3/10\n",
      "823/823 [==============================] - 1s 1ms/step - loss: 680163328.0000 - mse: 680163328.0000\n",
      "Epoch 4/10\n",
      "823/823 [==============================] - 1s 1ms/step - loss: 680163200.0000 - mse: 680163200.0000\n",
      "Epoch 5/10\n",
      "823/823 [==============================] - 1s 1ms/step - loss: 680162880.0000 - mse: 680162880.0000\n",
      "Epoch 6/10\n",
      "823/823 [==============================] - 1s 1ms/step - loss: 680163328.0000 - mse: 680163328.0000\n",
      "Epoch 7/10\n",
      "823/823 [==============================] - 1s 1ms/step - loss: 680162944.0000 - mse: 680162944.0000\n",
      "Epoch 8/10\n",
      "823/823 [==============================] - 1s 1ms/step - loss: 680163072.0000 - mse: 680163072.0000\n",
      "Epoch 9/10\n",
      "823/823 [==============================] - 1s 1ms/step - loss: 680163392.0000 - mse: 680163392.0000\n",
      "Epoch 10/10\n",
      "823/823 [==============================] - 1s 1ms/step - loss: 680163840.0000 - mse: 680163840.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28599957dc0>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [store_data['store_nm'].to_numpy(), \n",
    "store_data['province_cd'].to_numpy(),store_data['store_size_cd'].to_numpy(),store_data['store_concept_type_nm'].to_numpy(), store_data['shopping_centre_nm'].to_numpy(),store_data[numerical_cols].values\n",
    "]\n",
    "model.fit(inputs, y = store_data['sales_qty'].to_numpy(), epochs=10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the empedding layer, each value in the embedding layer represent the numberical value for the specific value of that category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506\n",
      "12\n",
      "6\n",
      "8\n",
      "93\n"
     ]
    }
   ],
   "source": [
    "print(len(model.layers[5].get_weights()[0]))\n",
    "print(len(model.layers[6].get_weights()[0]))\n",
    "print(len(model.layers[7].get_weights()[0]))\n",
    "print(len(model.layers[8].get_weights()[0]))\n",
    "print(len(model.layers[9].get_weights()[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save each embedding into excel, each row represent the vector representation of that unique value in that categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(\"embeddings/entity_cat_emb/\")\n",
    "except:\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = 5\n",
    "for i in categorical_cols:\n",
    "\n",
    "    # Get weights from the embedding layer\n",
    "    cat_emb_df = pd.DataFrame(model.layers[number].get_weights()[0]).reset_index()\n",
    "    # Add prefix to the embedding names\n",
    "    #cat_emb_df = cat_emb_df.add_prefix('cat_')\n",
    "    \n",
    "    # add category name in front for each cat_X column\n",
    "    cat_emb_df = cat_emb_df.rename(columns={c: i + \"_cat_\" + str(c) for c in cat_emb_df.columns if c not in [i, 'index']})\n",
    "\n",
    "    cat_encoder = {}\n",
    "    unique_cat = np.unique(store_data[i])\n",
    "\n",
    "    # Encode the categorical variable\n",
    "    for j in range(len(unique_cat)):\n",
    "        cat_encoder[unique_cat[j]] = j\n",
    "    # Put the categorical encoder dictionary into a dataframe\n",
    "    cat_encoder_df = pd.DataFrame(cat_encoder.items(), columns=[str(i), 'index'])\n",
    "\n",
    "    # Merge data to append the category name\n",
    "    cat_emb_df = pd.merge(cat_encoder_df, cat_emb_df, how = 'inner', on='index')\n",
    "    cat_emb_df = cat_emb_df.drop(columns=[\"index\"])\n",
    "\n",
    "    # Take a look at the data\n",
    "    cat_emb_df.head()\n",
    "    # Save embedding results\n",
    "    cat_emb_df.to_csv('embeddings/entity_cat_emb/'+str(i)+'_cat_embedding.csv', index = False)\n",
    "    number+=1\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map the categorical embeddings to their value in the original dataframe to complete the embedding for each store"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Circling back to the store_data dataframe, each categorical column has a vector representation for each of its unique values. We must map each unique value to its corresponding vector. For example, province_cd = 7 in the first row which was originally ON (Ontario). Now, we must map this value (7) to the corresponiding vector in province_cd_cat_embedding.csv: [0.017288119,0.56338453,-0.3758884]. Since it has 3 values, we will add 3 columns: province_cd_cat_0, province_cd_cat_1, province_cd_cat_2 to hold these values, and then remove the original province_cd column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_nm_cat_embedding = pd.read_csv('embeddings\\entity_cat_emb\\store_nm_cat_embedding.csv', header=0)\n",
    "province_cat_embedding = pd.read_csv('embeddings\\entity_cat_emb\\province_cd_cat_embedding.csv', header=0)\n",
    "store_size_cat_embedding = pd.read_csv('embeddings\\entity_cat_emb\\store_size_cd_cat_embedding.csv', header=0)\n",
    "store_concept_nm_cat_embedding = pd.read_csv('embeddings\\entity_cat_emb\\store_concept_type_nm_cat_embedding.csv', header=0)\n",
    "shopping_centre_cat_embedding = pd.read_csv('embeddings\\entity_cat_emb\\shopping_centre_nm_cat_embedding.csv', header=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge each categorical embedding above with store_data, to replace the respective categorical columns witht the vector that corresponds to each unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_sales_embedding = store_data.merge(store_nm_cat_embedding, on='store_nm', how='inner').drop(['store_nm'], axis=1)\n",
    "store_sales_embedding = store_sales_embedding.merge(province_cat_embedding, on='province_cd', how='inner').drop(['province_cd'], axis=1)\n",
    "store_sales_embedding = store_sales_embedding.merge(store_size_cat_embedding, on='store_size_cd', how='inner').drop(['store_size_cd'], axis=1)\n",
    "store_sales_embedding = store_sales_embedding.merge(store_concept_nm_cat_embedding, on='store_concept_type_nm', how='inner').drop(['store_concept_type_nm'], axis=1)\n",
    "store_sales_embedding = store_sales_embedding.merge(shopping_centre_cat_embedding, on='shopping_centre_nm', how='inner').drop(['shopping_centre_nm'], axis=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Sales Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, each embedding contains sales data per week per store, combined with store characteristics: numerical values and a concatenation of vector representations for its categorical values. This can be used in a forecasting model to predict weekly sales quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_sales_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(\"embeddings/store_embeddings/\")\n",
    "except:\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_sales_embedding.to_csv('embeddings/store_embeddings/store_sales_embedding.csv', index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain embeddings with store information alone (excluding sales data), we can extract it from the dataframe above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_embedding = store_sales_embedding.drop_duplicates(subset=\"store_num\")\n",
    "store_embedding = store_embedding.drop(columns=[\"yr_num\",\"wk_num\", \"sales_qty\"], axis=1)\n",
    "store_embedding = store_embedding.sort_values(by=\"store_num\")\n",
    "store_embedding.reset_index(inplace=True, drop=True )\n",
    "store_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_embedding.to_csv('embeddings/store_embeddings/store_embedding.csv', index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Store Sales Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These store embeddings cannot use the same evaluation model which predicted product type using the product embeddings. A forecasting model was created to predict sales using the store sales embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of how the sales data looks like, a scatter plot was created for the sales quantity per week for all stores. At any time of year, the sales quantity is likely within the range of [0 - 4000], but around week 20 and week 50 there are large spikes in sales. The highest observed sales quantities are over 100,000 during these times of year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(x=store_sales_embedding[\"wk_num\"], y=store_sales_embedding[\"sales_qty\"])\n",
    "ax.set_xlabel(\"week num\")\n",
    "ax.set_ylabel(\"sales quantity\")\n",
    "ax.set_title(\"sales quantity per week for all stores\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sales Forecasting Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sales quantity per week could depend on multiple factors. Next, we will see whether the embedded characteristics of any given store can help predict the sales quantity at various times of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Avril Emond\\anaconda3\\envs\\ctciter1\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1561: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV\n",
    "\n",
    "#shuffle the data\n",
    "df = store_sales_embedding.sample(frac=1)\n",
    "\n",
    "drop_for_X = [\"sales_qty\"]\n",
    "\n",
    "X = df.drop(columns=drop_for_X)\n",
    "Y = df[[\"sales_qty\"]]\n",
    "\n",
    "#obtain train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Lasso CV applies cross validation automatically\n",
    "linreg = LassoCV()\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the sales\n",
    "y_test_predictions = linreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train score is 0.606 and the test score is 0.603\n"
     ]
    }
   ],
   "source": [
    "# Evaluate models\n",
    "train_score = linreg.score(X_train, y_train)\n",
    "test_score = linreg.score(X_test, y_test)\n",
    "print(f'The train score is {train_score:.3f} and the test score is {test_score:.3f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forecasting model got an R^2 score of 0.606 on the train data and 0.603 on test data. The Store entity embeddings performed better than the one-hot encoded store embeddings, which got a R^2 score of 0.509 on test data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ctciter1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ba15d0a7ddc87a7cdc03ddbf3292318d97fc3790c5fcb6de245b479ad7e98a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
