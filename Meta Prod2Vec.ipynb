{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta-Prod2Vec Embedding\n",
    "Meta-prod2vec is a product recommendation algorithm that utilizes word2vec, a neural network-based technique for natural language processing, to generate product embeddings. These embeddings are then used to make recommendations based on the similarities between different products. The algorithm takes into account not only the product itself, but also its associated metadata, such as its category, brand, and customer reviews. This allows for more accurate and personalized recommendations, as it takes into account both the product's features and how it has been perceived by other customers. The algorithm can also be used to cluster products into groups based on their similarities, which can be useful for merchandising and product discovery.\n",
    "\n",
    "In the first version, we try to leverage side information including 'merch_division_nm','short_desc','merch_subcat_nm' data and use it to provide more information of prodct to product connection\n",
    "\n",
    "note that the file could talk +15 minute to run due to computational time on preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Tokenize the data and add it to the data list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_detail_detail_path = \"clean_data/cleaned_products_detailed.csv\"\n",
    "df_product_standard = pd.read_csv(product_detail_detail_path)\n",
    "df_detailed = df_product_standard[['ctr_product_num','attr_value_en_sentence']]\n",
    "df_detailed = df_detailed.dropna()\n",
    "df_detailed = df_detailed.drop_duplicates()\n",
    "df_detailed_subset = df_detailed.sample(frac=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input meta data including merch_division_nm,short_desc,merch_subcat_nm and target merch_lob_nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_standard = \"Data\\product\\product_standard_attributes.csv\" \n",
    "\n",
    "# put it in data farme\n",
    "df_product_standard = pd.read_csv(path_standard, low_memory=False) #lazyway to solve low memory issue\n",
    "df_right =df_product_standard[['ctr_product_num','merch_lob_nm','package_depth_qty','short_desc','package_height_qty','package_width_qty','package_volume_qty','package_weight_qty']]\n",
    "\n",
    "df_right['ctr_product_num'] = pd.to_numeric(df_right['ctr_product_num'], errors='coerce')\n",
    "df_right = df_right.dropna(subset=['ctr_product_num'])\n",
    "df_right['ctr_product_num'] = df_right['ctr_product_num'].astype('int')\n",
    "df_detailed_subset = df_detailed_subset.merge(df_right, on='ctr_product_num', how='left')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check label imbalance, undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "cnt_pro = df_detailed_subset['merch_lob_nm'].value_counts()\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.barplot(x = cnt_pro.index, y = cnt_pro.values, alpha=0.8)\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('type', fontsize=12)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nMax = 2500 #change to 2500\n",
    "\n",
    "# df_detailed_subset = df_detailed_subset.groupby('merch_lob_nm').apply(lambda x: x.sample(n=min(nMax, len(x))))\n",
    "\n",
    "# #print(df_detailed_subset['merch_lob_nm'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_detailed_subset = df_detailed_subset[df_detailed_subset.attr_value_en_sentence != 'Na']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\users\\stephen\\miniconda3\\envs\\ctc\\lib\\site-packages (4.9.2)\n"
     ]
    }
   ],
   "source": [
    "# ! pip install lxml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Stephen\\AppData\\Local\\Temp\\ipykernel_14716\\3733820858.py:4: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text,  \"html.parser\").text\n"
     ]
    }
   ],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "# import re\n",
    "# def cleanText(text):\n",
    "#     text = BeautifulSoup(text,  \"html.parser\").text\n",
    "#     text = re.sub(r'\\|\\|\\|', r' ', text) \n",
    "#     text = re.sub(r'http\\S+', r'<URL>', text)\n",
    "#     text = text.lower()\n",
    "#     text = text.replace('x', '')\n",
    "#     text = text.replace(',', ' ')\n",
    "#     return text\n",
    "# df_detailed_subset['attr_value_en_sentence'] = df_detailed_subset['attr_value_en_sentence'].apply(cleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_detailed_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_pro = df_detailed_subset['merch_lob_nm'].value_counts()\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.barplot(x = cnt_pro.index, y = cnt_pro.values, alpha=0.8)\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('type', fontsize=12)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create input data, append meta data together with product description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_right =df_product_standard[['ctr_product_num','merch_lob_nm','package_depth_qty','short_desc','package_height_qty','package_width_qty','package_volume_qty','package_weight_qty']]\n",
    "\n",
    "\n",
    "docLabels = []\n",
    "# getting all the names of all json files\n",
    "docLabels = [f for f in df_detailed_subset['package_depth_qty']]\n",
    "\n",
    "sdecLabels = []\n",
    "# getting all the names of all json files\n",
    "sdecLabels = [f for f in df_detailed_subset['short_desc']]\n",
    "\n",
    "subCatLabels = []\n",
    "# getting all the names of all json files\n",
    "subCatLabels = [f for f in df_detailed_subset['package_height_qty']]\n",
    "widthLabels = [f for f in df_detailed_subset['package_width_qty']]\n",
    "volumnLabels = [f for f in df_detailed_subset['package_volume_qty']]\n",
    "weightLabels = [f for f in df_detailed_subset['package_weight_qty']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_detailed_subset['attr_value_en_sentence'][0] + 'division_Name: ' + docLabels[0] + ' '+ 'short_description: ' + sdecLabels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "j = 0\n",
    "for i, row in df_detailed_subset.iterrows():\n",
    "    #print(row['attr_value_en_sentence'])\n",
    "    #data.append(row['attr_value_en_sentence'].split())\n",
    "    output = 'package depth is ' + str(docLabels[j]) + ' ' + 'short_description is ' + str(sdecLabels[j]) + ' ' + 'package height is ' + str(subCatLabels[j]) + ' '+'package width is ' + str(widthLabels[j]) + ' '+'package volumn is ' + str(volumnLabels[j]) + ' ' +'package weight is ' + str(weightLabels[j]) + ' ' + 'full description is ' + str(row['attr_value_en_sentence'])\n",
    "    data.append(output.split())\n",
    "    j+=1\n",
    "# Train the model\n",
    "#model = Word2Vec(data, min_count=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Doc2Vec embedding with input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "#dataset = api.load(\"text8\")\n",
    "#data = [d for d in dataset]\n",
    "def tagged_document(list_of_list_of_words):\n",
    "   for i, list_of_words in enumerate(list_of_list_of_words):\n",
    "      yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n",
    "data_for_training = list(tagged_document(data))\n",
    "#print(data_for_training[:1])\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=50)\n",
    "model.build_vocab(data_for_training)\n",
    "model.train(data_for_training, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "#print(model.infer_vector(['violent', 'means', 'to', 'destroy', 'the','organization']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = []\n",
    "for i in data:\n",
    "    value.append(model.infer_vector(i,epochs=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model.infer_vector(['Travel', 'poker', 'chips']))\n",
    "value[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save embedding to csv to input into evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embedding = pd.DataFrame(value,index = df_detailed_subset['ctr_product_num'])\n",
    "df_embedding.to_csv(\"embeddings/meta_prod2vec_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_product_detailed = pd.read_csv('clean_data/cleaned_products_detailed.csv')\n",
    "df_product_standard = pd.read_csv('clean_data/cleaned_products.csv')\n",
    "\n",
    "df_product_standard = df_product_standard[[\"ctr_product_num\", \"merch_lob_nm\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on based MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring meta_prod2vec_all.csv\n",
      "MLP_3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import KFold, cross_validate, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, top_k_accuracy_score, f1_score\n",
    "\n",
    "\n",
    "\n",
    "def score_cv(model, X, y, k = 0):\n",
    "    k_folds = KFold(n_splits = k)\n",
    "    #acc = None\n",
    "    #acc = cross_val_score(model, X, y, cv=k_folds, n_jobs= -1)\n",
    "    #f1_macro = cross_val_score(model, X, y, scoring=\"f1_macro\", cv=k_folds, n_jobs= -1)\n",
    "\n",
    "    scoring = {\"Accuracy\": make_scorer(accuracy_score),\n",
    "               \"Top_K\": make_scorer(top_k_accuracy_score),\n",
    "               \"F1 Score\": make_scorer(f1_score, average='weighted')}\n",
    "\n",
    "    scores = cross_validate(model, X, y, cv=k_folds, n_jobs= -1, return_train_score=True)\n",
    "\n",
    "    return scores, None\n",
    "\n",
    "results = []\n",
    "\n",
    "filename = 'meta_prod2vec_all.csv'\n",
    "print(\"Scoring\", filename)\n",
    "test_embedding = pd.read_csv(\"embeddings/\" + filename, index_col=0, header=0)\n",
    "#test_embedding = test_embedding[:500]\n",
    "df_combined = test_embedding.join(df_product_standard.set_index(\"ctr_product_num\"))\n",
    "\n",
    "df_combined.dropna(inplace=True)\n",
    "df_combined.merch_lob_nm = pd.Categorical(pd.factorize(df_combined.merch_lob_nm)[0])\n",
    "\n",
    "drop_for_X = [\"merch_lob_nm\"]\n",
    "\n",
    "X = df_combined.drop(columns=drop_for_X)\n",
    "Y = df_combined[[\"merch_lob_nm\"]]\n",
    "\n",
    "model = (\"MLP_3\", MLPClassifier(hidden_layer_sizes=(150, 100, 50), random_state=1, max_iter=300))\n",
    "print(model[0])\n",
    "acc, f1_micro = score_cv(model[1], X, Y, k=5)\n",
    "results.append((filename, model[0], [acc, f1_micro]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined\n",
    "cnt_pro = df_combined['merch_lob_nm'].value_counts()\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.barplot(x = cnt_pro.index, y = cnt_pro.values, alpha=0.8)\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('type', fontsize=12)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_prod2vec_all.csv\n",
      "fit_time [719.00783062 708.2864418  650.52785873 718.60972118 721.54478788]\n",
      "score_time [0.23257327 0.22707915 0.23440242 0.22441101 0.21957707]\n",
      "test_score [0.59222286 0.59672362 0.59026045 0.59403596 0.59384399]\n",
      "train_score [0.67894776 0.67955035 0.67760393 0.6734178  0.67542288]\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    print(result[0])\n",
    "    for i in result[2][0]:\n",
    "        print(i, result[2][0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "label = list(df_embedding.index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to pickle for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"embeddings/prod2vec_embedding_value.data\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(value, fp)\n",
    "with open(\"embeddings/prod2vec_embedding_label.data\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(label, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa1c97e2a2bb368dacb276be782706b41cdcfacde42bc327467a26ff10558e20"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
