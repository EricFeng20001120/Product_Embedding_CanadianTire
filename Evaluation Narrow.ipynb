{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation merch_bus_cat_nm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import KFold, cross_validate, cross_val_score, train_test_split\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, plot_confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_product_detailed = pd.read_csv('clean_data/cleaned_products_detailed.csv')\n",
    "df_product_standard = pd.read_csv('clean_data/cleaned_products.csv')\n",
    "\n",
    "df_product_standard = df_product_standard[[\"ctr_product_num\", \"merch_bus_cat_nm\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Random State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions_train = model.predict(X_train)\n",
    "    predictions_test = model.predict(X_test)\n",
    "\n",
    "    return [predictions_train, predictions_test]\n",
    "\n",
    "def get_metrics(y_train, y_test, predictions_train, predictions_test):\n",
    "    \n",
    "    metrics_dict = {\n",
    "        \"Train Accuracy Score\": accuracy_score(y_train, predictions_train),\n",
    "        \"Test Accuracy Score\": accuracy_score(y_test, predictions_test),\n",
    "        \"Train F1 Macro Score\": f1_score(y_train, predictions_train, average=\"macro\"),\n",
    "        \"Test F1 Macro Score\": f1_score(y_test, predictions_test, average=\"macro\"),\n",
    "        \"Train Precision Macro Score\": precision_score(y_train, predictions_train, average=\"macro\"),\n",
    "        \"Test Precision Macro Score\": precision_score(y_test, predictions_test, average=\"macro\"),\n",
    "        \"Train Recall Macro Score\": recall_score(y_train, predictions_train, average=\"macro\"),\n",
    "        \"Test Recall Macro Score\": recall_score(y_test, predictions_test, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "    return metrics_dict\n",
    "\n",
    "def score_cv(model, X, y, k = 5):\n",
    "    k_folds = KFold(n_splits = k)\n",
    "    #acc = None\n",
    "    #acc = cross_val_score(model, X, y, cv=k_folds, n_jobs= -1)\n",
    "    #f1_macro = cross_val_score(model, X, y, scoring=\"f1_macro\", cv=k_folds, n_jobs= -1)\n",
    "\n",
    "    scoring = [\"precision_macro\", \"recall_macro\", \"f1_macro\", \"accuracy\"]\n",
    "\n",
    "    scores = cross_validate(model, X, y, cv=k_folds, n_jobs= -1, return_train_score=True, error_score=\"raise\", scoring=scoring)\n",
    "\n",
    "    return scores, None\n",
    "\n",
    "def print_results(results):\n",
    "    for result in results:\n",
    "        print(result[0])\n",
    "        for i in result[2][0]:\n",
    "            print(i, [round(x, 2) for x in result[2][0][i]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_models = [\n",
    "    (\"MLP_3\", MLPClassifier(hidden_layer_sizes=(150, 100, 50), random_state=1, max_iter=300)),\n",
    "]\n",
    "\n",
    "all_files = os.listdir(\"embeddings\")    \n",
    "embeddings = list(filter(lambda f: f.endswith('.csv'), all_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i, filename in enumerate(embeddings):\n",
    "    test_embedding = pd.read_csv(\"embeddings/\" + filename, index_col=0, header=0)\n",
    "    test_embedding = test_embedding.sample(frac=1)[:100000]\n",
    "    df_combined = test_embedding.join(df_product_standard.set_index(\"ctr_product_num\"))\n",
    "    df_combined.dropna(inplace=True)\n",
    "\n",
    "    drop_for_X = [\"merch_bus_cat_nm\"]\n",
    "\n",
    "    X = df_combined.drop(columns=drop_for_X)\n",
    "    Y = df_combined[[\"merch_bus_cat_nm\"]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "\n",
    "    for model in evaluation_models:\n",
    "        results.append([filename] + score_model(model[1], X_train, X_test, y_train, y_test) + [X_train, X_test, y_train, y_test])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_metrics = {}\n",
    "# for result in results:\n",
    "#     cm = confusion_matrix(result[6], result[2])\n",
    "#     cm_display = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = np.unique(result[6]))\n",
    "#     fig, ax = plt.subplots(figsize=(15, 15))\n",
    "#     cm_display.plot(xticks_rotation =\"vertical\", include_values=False, ax=ax)\n",
    "#     plt.title(result[0] + \" narrow\")\n",
    "#     plt.savefig(\"results/\"+result[0].split(\".\")[0]+\"_narrow.png\", facecolor=\"white\", bbox_inches='tight')\n",
    "#     result_metrics[result[0]] = get_metrics(result[5], result[6], result[1], result[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results:\n",
    "    result_metrics[result[0]] = get_metrics(result[5], result[6], result[1], result[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame.from_dict(result_metrics, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"results/metrics_narrow.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "39b28e74a53c7fe6627b99d166fc22c7c646315a2e88d31074bfa870e9a41665"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
