{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Tokenize the data and add it to the data list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_detail_path = \"clean_data/cleaned_products_detailed.csv\"\n",
    "df_product_detailed = pd.read_csv(product_detail_path)\n",
    "df_detailed = df_product_detailed[['ctr_product_num','attr_value_en_sentence']]\n",
    "df_detailed = df_detailed.dropna()\n",
    "df_detailed = df_detailed.drop_duplicates()\n",
    "df_detailed_subset = df_detailed.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235418, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_detailed_subset.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the input data is a list of vocabulary corresponding to each \"ctr_product_num\". each word will gain a vector representation after word2vec, and in order to convert to doc2vec, we average the vector value for all words in that product description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i, row in df_detailed_subset.iterrows():\n",
    "    data.append(row['attr_value_en_sentence'].split())\n",
    "\n",
    "print(data[0])\n",
    "df_detailed_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = Word2Vec(data, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to generate product embeddings\n",
    "product_embeddings = {}\n",
    "for i, row in df_detailed_subset.iterrows():\n",
    "    # Generate an embedding for the entire product sentence by concatenating the individual word embeddings\n",
    "    product_embedding = np.mean([model.wv[word] for word in row['attr_value_en_sentence'].split()], axis=0)\n",
    "    product_embeddings[row['ctr_product_num']] = product_embedding\n",
    "\n",
    "embeddings_dict = pd.DataFrame.from_dict(product_embeddings, orient='index')\n",
    "embeddings_dict.index.names = [\"ctr_product_num\"]\n",
    "\n",
    "embeddings_dict.to_csv(\"embeddings/prod2vec_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Extract the product embeddings from the product_embeddings dictionary\n",
    "product_embeddings_list = np.array(list(product_embeddings.values()))\n",
    "\n",
    "# Use t-SNE to reduce the dimensions of the product embeddings\n",
    "tsne = TSNE(n_components=3)\n",
    "product_embeddings_3d = tsne.fit_transform(product_embeddings_list)\n",
    "\n",
    "# Extract the product names from the product_embeddings dictionary\n",
    "product_names = list(product_embeddings.keys())\n",
    "\n",
    "# Create a dataframe with the product embeddings and names\n",
    "df = pd.DataFrame({'x': product_embeddings_3d[:, 0], 'y': product_embeddings_3d[:, 1], 'z': product_embeddings_3d[:, 2], 'product': product_names})\n",
    "\n",
    "# Plot the product embeddings in 3D using plotly\n",
    "fig = px.scatter_3d(df, x='x', y='y', z='z', text='product')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_standard = \"Data\\product\\product_standard_attributes.csv\" \n",
    "\n",
    "# put it in data farme\n",
    "df_product_standard = pd.read_csv(path_standard, low_memory=False) #lazyway to solve low memory issue\n",
    "df_right =df_product_standard[['ctr_product_num','short_desc']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_right['ctr_product_num'] = pd.to_numeric(df_right['ctr_product_num'], errors='coerce')\n",
    "df_right = df_right.dropna(subset=['ctr_product_num'])\n",
    "df_right['ctr_product_num'] = df_right['ctr_product_num'].astype('int')\n",
    "df_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_desc = df_detailed_subset.merge(df_right, on='ctr_product_num', how='left')\n",
    "df_desc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df_desc.iterrows():\n",
    "    # Generate an embedding for the entire product sentence by concatenating the individual word embeddings\n",
    "    product_embedding = np.mean([model.wv[word] for word in row['attr_value_en_sentence'].split()], axis=0)\n",
    "    product_embeddings[row['ctr_product_num']] = product_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Labels separately on a line-by-line manner.\n",
    "with open('visualization/metadata.tsv', \"w\") as f:\n",
    "  for i, row in df_desc.iterrows():\n",
    "    f.write(\"{}\\n\".format(row['short_desc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tfflow\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorboard.plugins import projector\n",
    "# Extract the product embeddings from the product_embeddings dictionary\n",
    "product_embeddings_list = list(product_embeddings.values())\n",
    "\n",
    "# Convert the product embeddings to a numpy array\n",
    "product_embeddings_array = np.array(product_embeddings_list, dtype=np.float32)\n",
    "\n",
    "# Use t-SNE to reduce the dimensions of the product embeddings\n",
    "tsne = TSNE(perplexity=30,n_components=3)\n",
    "product_embeddings_3d = tsne.fit_transform(product_embeddings_array)\n",
    "#print(product_embeddings_array)\n",
    "#print(product_embeddings_3d)\n",
    "# Save the dataset to a file\n",
    "with open('visualization/product_embeddings_3d.tsv', 'w') as f:\n",
    "    for x, y, z in product_embeddings_3d:\n",
    "        f.write(f\"{x}\\t{y}\\t{z}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ctc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "713c6a8077557bf6b12f32534318771ae7cac7046ada6a98be74843782a407b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
