{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Tokenize the data and add it to the data list\n",
    "\n",
    "import os\n",
    "import openai\n",
    "import wandb\n",
    "\n",
    "from openai.embeddings_utils import get_embedding\n",
    "# Ensure you have your API key set in your environment per the README: https://github.com/openai/openai-python#usage\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(\"embeddings\")\n",
    "except:\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_detail_detail_path = \"clean_data/cleaned_products_detailed.csv\"\n",
    "product_standard_path = \"clean_data/cleaned_products.csv\"\n",
    "\n",
    "df_product_detail = pd.read_csv(product_detail_detail_path)\n",
    "df_product_standard = pd.read_csv(product_standard_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_detailed = df_product_detail[['ctr_product_num','attr_value_en_sentence']]\n",
    "df_detailed = df_detailed.drop_duplicates()\n",
    "df_detailed = df_detailed.dropna()\n",
    "df_detailed_subset = df_detailed.sample(frac=1)[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235418, 2)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_detailed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = \"_100k\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only a subset of 100k samples is used due to hardware and computational limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ctr_product_num</th>\n",
       "      <th>attr_value_en_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5044</td>\n",
       "      <td>Travel poker chips</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5045</td>\n",
       "      <td>40 piece poker chips</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>22726</td>\n",
       "      <td>General Tire GMAX UHP; Features: Wide circumfe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>31702</td>\n",
       "      <td>Top 3 Vehicle Applications: Toyota Tercel (199...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>31703</td>\n",
       "      <td>Top 3 Vehicle Applications: Hyundai Accent (20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1559079</th>\n",
       "      <td>8997335</td>\n",
       "      <td>Reliable performance through consistency and u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1559082</th>\n",
       "      <td>8997336</td>\n",
       "      <td>Reliable performance through consistency and u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1559085</th>\n",
       "      <td>8997337</td>\n",
       "      <td>Made from heavy duty 600 denier water-resistan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1559091</th>\n",
       "      <td>8997338</td>\n",
       "      <td>Strong 600D polyester exteriorRemovable divide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1559098</th>\n",
       "      <td>8997339</td>\n",
       "      <td>Heavy-duty 600D Polyester and WR-coated materi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>235418 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ctr_product_num                             attr_value_en_sentence\n",
       "1                   5044                                 Travel poker chips\n",
       "2                   5045                               40 piece poker chips\n",
       "6                  22726  General Tire GMAX UHP; Features: Wide circumfe...\n",
       "16                 31702  Top 3 Vehicle Applications: Toyota Tercel (199...\n",
       "19                 31703  Top 3 Vehicle Applications: Hyundai Accent (20...\n",
       "...                  ...                                                ...\n",
       "1559079          8997335  Reliable performance through consistency and u...\n",
       "1559082          8997336  Reliable performance through consistency and u...\n",
       "1559085          8997337  Made from heavy duty 600 denier water-resistan...\n",
       "1559091          8997338  Strong 600D polyester exteriorRemovable divide...\n",
       "1559098          8997339  Heavy-duty 600D Polyester and WR-coated materi...\n",
       "\n",
       "[235418 rows x 2 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_detailed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is from the product detailed dataset. The team has merged all the attributes of each product to become paragraph. Sample instance of the data will be displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ctr_product_num</th>\n",
       "      <th>ctr_style_name</th>\n",
       "      <th>short_desc</th>\n",
       "      <th>long_desc</th>\n",
       "      <th>merch_division_nm</th>\n",
       "      <th>merch_lob_nm</th>\n",
       "      <th>merch_bus_cat_nm</th>\n",
       "      <th>merch_subcat_nm</th>\n",
       "      <th>merch_fineline_nm</th>\n",
       "      <th>...</th>\n",
       "      <th>ctr_product_profile_cd</th>\n",
       "      <th>ctr_consumer_role_cd</th>\n",
       "      <th>package_depth_qty</th>\n",
       "      <th>package_height_qty</th>\n",
       "      <th>package_width_qty</th>\n",
       "      <th>package_volume_qty</th>\n",
       "      <th>package_weight_qty</th>\n",
       "      <th>national_consumer_price_amt</th>\n",
       "      <th>cold_sensitive_ind</th>\n",
       "      <th>heat_sensitive_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>232151</th>\n",
       "      <td>233862</td>\n",
       "      <td>73603</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P235/55R17 PIRP8FSUV</td>\n",
       "      <td>235/55R17 103V XL Pirelli P8FSUV.</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>TIRES</td>\n",
       "      <td>ALL SEASON TIRES</td>\n",
       "      <td>All Season Passenger &amp; CUV Tires</td>\n",
       "      <td>Pirelli P8</td>\n",
       "      <td>...</td>\n",
       "      <td>JOB_JOY</td>\n",
       "      <td>DESTINATION</td>\n",
       "      <td>27.2</td>\n",
       "      <td>9.3</td>\n",
       "      <td>27.2</td>\n",
       "      <td>3.95453</td>\n",
       "      <td>29.572</td>\n",
       "      <td>265.99</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  ctr_product_num ctr_style_name            short_desc  \\\n",
       "232151      233862            73603            NaN  P235/55R17 PIRP8FSUV   \n",
       "\n",
       "                                long_desc merch_division_nm merch_lob_nm  \\\n",
       "232151  235/55R17 103V XL Pirelli P8FSUV.        AUTOMOTIVE        TIRES   \n",
       "\n",
       "        merch_bus_cat_nm                   merch_subcat_nm merch_fineline_nm  \\\n",
       "232151  ALL SEASON TIRES  All Season Passenger & CUV Tires        Pirelli P8   \n",
       "\n",
       "        ... ctr_product_profile_cd ctr_consumer_role_cd package_depth_qty  \\\n",
       "232151  ...                JOB_JOY          DESTINATION              27.2   \n",
       "\n",
       "       package_height_qty package_width_qty package_volume_qty  \\\n",
       "232151                9.3              27.2            3.95453   \n",
       "\n",
       "        package_weight_qty  national_consumer_price_amt  cold_sensitive_ind  \\\n",
       "232151              29.572                       265.99                   N   \n",
       "\n",
       "        heat_sensitive_ind  \n",
       "232151                   Y  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_product_standard[df_product_standard.ctr_product_num == 73603]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['STABLE AND PRECISE HANDLING - The reinforced shoulder blocks and transversal grooves on central ribs provide a high stability. SAFE DRIVING - The high density tread sipes and internal shoulder blocks provide and excellent all-season grip. HIGHER MILEAGE - Compound materials developed with optimized polymer blend promote a long lasting experience']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_detailed[df_detailed.ctr_product_num == 73603].attr_value_en_sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different product but product detail is identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ctr_product_num</th>\n",
       "      <th>ctr_style_name</th>\n",
       "      <th>short_desc</th>\n",
       "      <th>long_desc</th>\n",
       "      <th>merch_division_nm</th>\n",
       "      <th>merch_lob_nm</th>\n",
       "      <th>merch_bus_cat_nm</th>\n",
       "      <th>merch_subcat_nm</th>\n",
       "      <th>merch_fineline_nm</th>\n",
       "      <th>...</th>\n",
       "      <th>ctr_product_profile_cd</th>\n",
       "      <th>ctr_consumer_role_cd</th>\n",
       "      <th>package_depth_qty</th>\n",
       "      <th>package_height_qty</th>\n",
       "      <th>package_width_qty</th>\n",
       "      <th>package_volume_qty</th>\n",
       "      <th>package_weight_qty</th>\n",
       "      <th>national_consumer_price_amt</th>\n",
       "      <th>cold_sensitive_ind</th>\n",
       "      <th>heat_sensitive_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>210813</th>\n",
       "      <td>212375</td>\n",
       "      <td>73600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P215/45R17 PIR P8FSP</td>\n",
       "      <td>215/45R17 91V XL Pirelli P8FSP.</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>TIRES</td>\n",
       "      <td>ALL SEASON TIRES</td>\n",
       "      <td>All Season Passenger &amp; CUV Tires</td>\n",
       "      <td>Pirelli P8</td>\n",
       "      <td>...</td>\n",
       "      <td>JOB_JOY</td>\n",
       "      <td>DESTINATION</td>\n",
       "      <td>24.6</td>\n",
       "      <td>8.5</td>\n",
       "      <td>24.6</td>\n",
       "      <td>2.968853</td>\n",
       "      <td>21.563</td>\n",
       "      <td>240.99</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  ctr_product_num ctr_style_name            short_desc  \\\n",
       "210813      212375            73600            NaN  P215/45R17 PIR P8FSP   \n",
       "\n",
       "                              long_desc merch_division_nm merch_lob_nm  \\\n",
       "210813  215/45R17 91V XL Pirelli P8FSP.        AUTOMOTIVE        TIRES   \n",
       "\n",
       "        merch_bus_cat_nm                   merch_subcat_nm merch_fineline_nm  \\\n",
       "210813  ALL SEASON TIRES  All Season Passenger & CUV Tires        Pirelli P8   \n",
       "\n",
       "        ... ctr_product_profile_cd ctr_consumer_role_cd package_depth_qty  \\\n",
       "210813  ...                JOB_JOY          DESTINATION              24.6   \n",
       "\n",
       "       package_height_qty package_width_qty package_volume_qty  \\\n",
       "210813                8.5              24.6           2.968853   \n",
       "\n",
       "        package_weight_qty  national_consumer_price_amt  cold_sensitive_ind  \\\n",
       "210813              21.563                       240.99                   N   \n",
       "\n",
       "        heat_sensitive_ind  \n",
       "210813                   Y  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_product_standard[df_product_standard.ctr_product_num == 73600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SAFE DRIVING - The silica enhanced compound provides a responsive handling and an excellent traction PRECISE STEERING - The stiffer tire carcass structure make for an efficient handling and steering ENHANCED BRAKING PERFORMANCE AND OUTSTANDING DRIVING COMFORT - The optimized pitch sequencing provides an exceptional braking performance and a quiet drive OUTSTANDING GRIP ON WET - Improved lateral stability due to central grooves']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_detailed[df_detailed.ctr_product_num == 73600].attr_value_en_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ctr_product_num</th>\n",
       "      <th>ctr_style_name</th>\n",
       "      <th>short_desc</th>\n",
       "      <th>long_desc</th>\n",
       "      <th>merch_division_nm</th>\n",
       "      <th>merch_lob_nm</th>\n",
       "      <th>merch_bus_cat_nm</th>\n",
       "      <th>merch_subcat_nm</th>\n",
       "      <th>merch_fineline_nm</th>\n",
       "      <th>...</th>\n",
       "      <th>ctr_product_profile_cd</th>\n",
       "      <th>ctr_consumer_role_cd</th>\n",
       "      <th>package_depth_qty</th>\n",
       "      <th>package_height_qty</th>\n",
       "      <th>package_width_qty</th>\n",
       "      <th>package_volume_qty</th>\n",
       "      <th>package_weight_qty</th>\n",
       "      <th>national_consumer_price_amt</th>\n",
       "      <th>cold_sensitive_ind</th>\n",
       "      <th>heat_sensitive_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>714569</th>\n",
       "      <td>720508</td>\n",
       "      <td>73601</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P225/45R17 PIR P8FSP</td>\n",
       "      <td>225/45R17 94W XL Pirelli P8FSP.</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>TIRES</td>\n",
       "      <td>ALL SEASON TIRES</td>\n",
       "      <td>All Season Passenger &amp; CUV Tires</td>\n",
       "      <td>Pirelli P8</td>\n",
       "      <td>...</td>\n",
       "      <td>JOB_JOY</td>\n",
       "      <td>DESTINATION</td>\n",
       "      <td>25.0</td>\n",
       "      <td>8.9</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.196676</td>\n",
       "      <td>21.999</td>\n",
       "      <td>244.99</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  ctr_product_num ctr_style_name            short_desc  \\\n",
       "714569      720508            73601            NaN  P225/45R17 PIR P8FSP   \n",
       "\n",
       "                              long_desc merch_division_nm merch_lob_nm  \\\n",
       "714569  225/45R17 94W XL Pirelli P8FSP.        AUTOMOTIVE        TIRES   \n",
       "\n",
       "        merch_bus_cat_nm                   merch_subcat_nm merch_fineline_nm  \\\n",
       "714569  ALL SEASON TIRES  All Season Passenger & CUV Tires        Pirelli P8   \n",
       "\n",
       "        ... ctr_product_profile_cd ctr_consumer_role_cd package_depth_qty  \\\n",
       "714569  ...                JOB_JOY          DESTINATION              25.0   \n",
       "\n",
       "       package_height_qty package_width_qty package_volume_qty  \\\n",
       "714569                8.9              25.0           3.196676   \n",
       "\n",
       "        package_weight_qty  national_consumer_price_amt  cold_sensitive_ind  \\\n",
       "714569              21.999                       244.99                   N   \n",
       "\n",
       "        heat_sensitive_ind  \n",
       "714569                   Y  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_product_standard[df_product_standard.ctr_product_num == 73601]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SAFE DRIVING - The silica enhanced compound provides a responsive handling and an excellent traction PRECISE STEERING - The stiffer tire carcass structure make for an efficient handling and steering ENHANCED BRAKING PERFORMANCE AND OUTSTANDING DRIVING COMFORT - The optimized pitch sequencing provides an exceptional braking performance and a quiet drive OUTSTANDING GRIP ON WET - Improved lateral stability due to central grooves']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_detailed[df_detailed.ctr_product_num == 73601].attr_value_en_sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar description types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ctr_product_num</th>\n",
       "      <th>ctr_style_name</th>\n",
       "      <th>short_desc</th>\n",
       "      <th>long_desc</th>\n",
       "      <th>merch_division_nm</th>\n",
       "      <th>merch_lob_nm</th>\n",
       "      <th>merch_bus_cat_nm</th>\n",
       "      <th>merch_subcat_nm</th>\n",
       "      <th>merch_fineline_nm</th>\n",
       "      <th>...</th>\n",
       "      <th>ctr_product_profile_cd</th>\n",
       "      <th>ctr_consumer_role_cd</th>\n",
       "      <th>package_depth_qty</th>\n",
       "      <th>package_height_qty</th>\n",
       "      <th>package_width_qty</th>\n",
       "      <th>package_volume_qty</th>\n",
       "      <th>package_weight_qty</th>\n",
       "      <th>national_consumer_price_amt</th>\n",
       "      <th>cold_sensitive_ind</th>\n",
       "      <th>heat_sensitive_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>517143</th>\n",
       "      <td>521166</td>\n",
       "      <td>31703</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*175/70R13 82T CWTRK</td>\n",
       "      <td>*175/70R13 82T Certified WinterTrek</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>TIRES</td>\n",
       "      <td>WINTER TIRES</td>\n",
       "      <td>Winter Passenger &amp; CUV Tires</td>\n",
       "      <td>Certified WinterTrek</td>\n",
       "      <td>...</td>\n",
       "      <td>JOB_JOY</td>\n",
       "      <td>DESTINATION</td>\n",
       "      <td>22.7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>22.7</td>\n",
       "      <td>2.073948</td>\n",
       "      <td>15.829</td>\n",
       "      <td>109.99</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  ctr_product_num ctr_style_name            short_desc  \\\n",
       "517143      521166            31703            NaN  *175/70R13 82T CWTRK   \n",
       "\n",
       "                                  long_desc merch_division_nm merch_lob_nm  \\\n",
       "517143  *175/70R13 82T Certified WinterTrek        AUTOMOTIVE        TIRES   \n",
       "\n",
       "       merch_bus_cat_nm               merch_subcat_nm     merch_fineline_nm  \\\n",
       "517143     WINTER TIRES  Winter Passenger & CUV Tires  Certified WinterTrek   \n",
       "\n",
       "        ... ctr_product_profile_cd ctr_consumer_role_cd package_depth_qty  \\\n",
       "517143  ...                JOB_JOY          DESTINATION              22.7   \n",
       "\n",
       "       package_height_qty package_width_qty package_volume_qty  \\\n",
       "517143                7.0              22.7           2.073948   \n",
       "\n",
       "        package_weight_qty  national_consumer_price_amt  cold_sensitive_ind  \\\n",
       "517143              15.829                       109.99                   N   \n",
       "\n",
       "        heat_sensitive_ind  \n",
       "517143                   N  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_product_standard[df_product_standard.ctr_product_num == 31703]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Top 3 Vehicle Applications: Hyundai Accent (2002), Hyundai Accent (2003), Hyundai Accent (2001) Studdable option']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_detailed[df_detailed.ctr_product_num == 31703].attr_value_en_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ctr_product_num</th>\n",
       "      <th>ctr_style_name</th>\n",
       "      <th>short_desc</th>\n",
       "      <th>long_desc</th>\n",
       "      <th>merch_division_nm</th>\n",
       "      <th>merch_lob_nm</th>\n",
       "      <th>merch_bus_cat_nm</th>\n",
       "      <th>merch_subcat_nm</th>\n",
       "      <th>merch_fineline_nm</th>\n",
       "      <th>...</th>\n",
       "      <th>ctr_product_profile_cd</th>\n",
       "      <th>ctr_consumer_role_cd</th>\n",
       "      <th>package_depth_qty</th>\n",
       "      <th>package_height_qty</th>\n",
       "      <th>package_width_qty</th>\n",
       "      <th>package_volume_qty</th>\n",
       "      <th>package_weight_qty</th>\n",
       "      <th>national_consumer_price_amt</th>\n",
       "      <th>cold_sensitive_ind</th>\n",
       "      <th>heat_sensitive_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>315007</th>\n",
       "      <td>317492</td>\n",
       "      <td>31702</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*155/80R13 79T CWTRK</td>\n",
       "      <td>*155/80R13 79T Certified WinterTrek</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>TIRES</td>\n",
       "      <td>WINTER TIRES</td>\n",
       "      <td>Winter Passenger &amp; CUV Tires</td>\n",
       "      <td>Certified WinterTrek</td>\n",
       "      <td>...</td>\n",
       "      <td>JOB_JOY</td>\n",
       "      <td>DESTINATION</td>\n",
       "      <td>22.8</td>\n",
       "      <td>6.2</td>\n",
       "      <td>22.8</td>\n",
       "      <td>1.852281</td>\n",
       "      <td>13.536</td>\n",
       "      <td>104.99</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  ctr_product_num ctr_style_name            short_desc  \\\n",
       "315007      317492            31702            NaN  *155/80R13 79T CWTRK   \n",
       "\n",
       "                                  long_desc merch_division_nm merch_lob_nm  \\\n",
       "315007  *155/80R13 79T Certified WinterTrek        AUTOMOTIVE        TIRES   \n",
       "\n",
       "       merch_bus_cat_nm               merch_subcat_nm     merch_fineline_nm  \\\n",
       "315007     WINTER TIRES  Winter Passenger & CUV Tires  Certified WinterTrek   \n",
       "\n",
       "        ... ctr_product_profile_cd ctr_consumer_role_cd package_depth_qty  \\\n",
       "315007  ...                JOB_JOY          DESTINATION              22.8   \n",
       "\n",
       "       package_height_qty package_width_qty package_volume_qty  \\\n",
       "315007                6.2              22.8           1.852281   \n",
       "\n",
       "        package_weight_qty  national_consumer_price_amt  cold_sensitive_ind  \\\n",
       "315007              13.536                       104.99                   N   \n",
       "\n",
       "        heat_sensitive_ind  \n",
       "315007                   N  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_product_standard[df_product_standard.ctr_product_num == 31702]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Top 3 Vehicle Applications: Toyota Tercel (1999), Hyundai Accent (2002), Toyota Tercel (1998) Studdable option']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_detailed[df_detailed.ctr_product_num == 31702].attr_value_en_sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now these data will be fed into the transformer based language model as a paragraph to generate embedding of the product based on the description."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ndef get_embedding(text, model=\"text-embedding-ada-002\"):\\n   text = text.replace(\"\\n\", \" \")\\n   return openai.Embedding.create(input = [text], model=model)[\\'data\\'][0][\\'embedding\\']\\n \\n# In order to use this we have to obtain a key.\\nopenai.api_key = os.environ.get(\\'OpenaiKey\\')\\n\\n'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']\n",
    " \n",
    "# In order to use this we have to obtain a key.\n",
    "openai.api_key = os.environ.get('OpenaiKey')\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndf_detailed['ada_similarity'] = df_detailed.attr_value_en_sentence.apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\\ndf_detailed['ada_search'] = df_detailed.attr_value_en_sentence.apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\\n\\n\""
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conclusion is in this way this doesn't work due to the fact that my account is free account. There might be a get away by controlling the rate of the embedding generation\n",
    "# But that possibility might explore later if this NLP is even possible or not.\n",
    "'''\n",
    "df_detailed['ada_similarity'] = df_detailed.attr_value_en_sentence.apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\n",
    "df_detailed['ada_search'] = df_detailed.attr_value_en_sentence.apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\n",
    "\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A limitation of the GPT model is paid services is required for our data size. To avoid paying we can try to adjust the rate but We'll try other language models like bert."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced language models\n",
    "\n",
    "Useful websites for reference:\n",
    "\n",
    "https://www.topbots.com/leading-nlp-language-models-2020/ \n",
    "\n",
    "https://medium.com/@nils_reimers/openai-gpt-3-text-embeddings-really-a-new-state-of-the-art-in-dense-text-embeddings-6571fe3ec9d9"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence-Transformers Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all-MiniLM-L6-v2\n",
    "\n",
    "all-MiniLM-L6-v2. An extremely small (80 MB) and fast model, with only 6 layers which producing embeddings with 384 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "MiniLM_L6_v2_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_embeddings = {}\n",
    "for i, row in df_detailed_subset.iterrows():\n",
    "    # Generate an embedding for the entire product sentence\n",
    "    generated_embedding = MiniLM_L6_v2_model.encode(row[\"attr_value_en_sentence\"])\n",
    "    generated_embeddings[row['ctr_product_num']] = generated_embedding\n",
    "embeddings_dict = pd.DataFrame.from_dict(generated_embeddings, orient='index')\n",
    "embeddings_dict.index.names = [\"ctr_product_num\"]\n",
    "\n",
    "embeddings_dict.to_csv(\"embeddings/minilm\"+ suffix + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_detailed_subset['all-MiniLM-L6-v2_embedding'] = df_detailed_subset.attr_value_en_sentence.apply(lambda x: MiniLM_L6_v2_model.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_detailed_subset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all-mpnet-base-v2\n",
    "\n",
    "all-mpnet-base-v2: A bert-base sized model (418 MB) with 12 layers and 768 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpnet_base_v2_model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_embeddings = {}\n",
    "for i, row in df_detailed_subset.iterrows():\n",
    "    # Generate an embedding for the entire sentence\n",
    "    generated_embedding = mpnet_base_v2_model.encode(row[\"attr_value_en_sentence\"])\n",
    "    generated_embeddings[row['ctr_product_num']] = generated_embedding\n",
    "embeddings_dict = pd.DataFrame.from_dict(generated_embeddings, orient='index')\n",
    "embeddings_dict.index.names = [\"ctr_product_num\"]\n",
    "\n",
    "embeddings_dict.to_csv(\"embeddings/mpnet\"+ suffix + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_detailed_subset['all-mpnet-base-v2_embedding'] = df_detailed_subset.attr_value_en_sentence.apply(lambda x: mpnet_base_v2_model.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_detailed_subset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all-roberta-large-v1\n",
    "\n",
    "all-roberta-large-v1: A model based on RoBERTA-large (1.3 GB) with 24 layers and 1024 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_large_v1_model = SentenceTransformer('sentence-transformers/all-roberta-large-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_embeddings = {}\n",
    "for i, row in df_detailed_subset.iterrows():\n",
    "    # Generate an embedding for the entire sentence\n",
    "    generated_embedding = roberta_large_v1_model.encode(row[\"attr_value_en_sentence\"])\n",
    "    generated_embeddings[row['ctr_product_num']] = generated_embedding\n",
    "embeddings_dict = pd.DataFrame.from_dict(generated_embeddings, orient='index')\n",
    "embeddings_dict.index.names = [\"ctr_product_num\"]\n",
    "\n",
    "embeddings_dict.to_csv(\"embeddings/roberta\"+ suffix + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_detailed_subset['all-roberta-large-v1_embedding'] = df_detailed_subset.attr_value_en_sentence.apply(lambda x: roberta_large_v1_model.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df_detailed_subset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google embedding model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence-T5\n",
    "Sentence-T5: The most recent text embedding model from Google published in August 2021.\n",
    "\n",
    "https://arxiv.org/pdf/2108.08877.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_t5_base_model = SentenceTransformer('sentence-transformers/sentence-t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_embeddings = {}\n",
    "for i, row in df_detailed_subset.iterrows():\n",
    "    # Generate an embedding for the entire sentence\n",
    "    generated_embedding = sentence_t5_base_model.encode(row[\"attr_value_en_sentence\"])\n",
    "    generated_embeddings[row['ctr_product_num']] = generated_embedding\n",
    "embeddings_dict = pd.DataFrame.from_dict(generated_embeddings, orient='index')\n",
    "embeddings_dict.index.names = [\"ctr_product_num\"]\n",
    "\n",
    "embeddings_dict.to_csv(\"embeddings/t5\"+ suffix + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_detailed_subset['sentence-t5-base_embedding'] = df_detailed_subset.attr_value_en_sentence.apply(lambda x: sentence_t5_base_model.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_detailed_subset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask Language Model task"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from transformers import BertForMaskedLM, DistilBertForMaskedLM\n",
    "from transformers import BertTokenizer, DistilBertTokenizer\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from tokenizers import BertWordPieceTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMS\n",
    "SEED_SPLIT = 0\n",
    "SEED_TRAIN = 0\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "EVAL_BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-5 \n",
    "LR_WARMUP_STEPS = 100\n",
    "WEIGHT_DECAY = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "dtf_mlm = df_detailed\n",
    "#dtf_mlm = dtf_mlm.rename(columns={\"review_content\": \"text\"})\n",
    "\n",
    "# Train/Valid Split\n",
    "df_train, df_valid = train_test_split(\n",
    "    dtf_mlm, test_size=0.15, random_state=SEED_SPLIT\n",
    ")\n",
    "\n",
    "len(df_train), len(df_valid)\n",
    "\n",
    "\n",
    "# Convert to Dataset object\n",
    "train_dataset = Dataset.from_pandas(df_train[['attr_value_en_sentence']].dropna())\n",
    "valid_dataset = Dataset.from_pandas(df_valid[['attr_value_en_sentence']].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "bert-base-uncased  # 12-layer, 768-hidden, 12-heads, 109M parameters\n",
    "distilbert-base-uncased  # 6-layer, 768-hidden, 12-heads, 65M parameters\n",
    "'''\n",
    "\n",
    "MODEL = 'bert'\n",
    "bert_type = 'bert-base-cased'\n",
    "\n",
    "if MODEL == 'distilbert':\n",
    "    TokenizerClass = DistilBertTokenizer \n",
    "    ModelClass = DistilBertForMaskedLM \n",
    "elif MODEL == 'bert':\n",
    "    TokenizerClass = BertTokenizer\n",
    "    ModelClass = BertForMaskedLM \n",
    "elif MODEL == 'roberta':\n",
    "    TokenizerClass = RobertaTokenizer\n",
    "    ModelClass = RobertaForMaskedLM\n",
    "elif MODEL == 'scibert':\n",
    "    TokenizerClass = AutoTokenizer\n",
    "    ModelClass = AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = TokenizerClass.from_pretrained(\n",
    "            bert_type, use_fast=True, do_lower_case=False, max_len=MAX_SEQ_LEN\n",
    "            )\n",
    "\n",
    "model = ModelClass.from_pretrained(bert_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\Stephen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\multiprocess\\pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"c:\\Users\\Stephen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 562, in wrapper\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n  File \"c:\\Users\\Stephen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 529, in wrapper\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n  File \"c:\\Users\\Stephen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\fingerprint.py\", line 480, in wrapper\n    out = func(self, *args, **kwargs)\n  File \"c:\\Users\\Stephen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 3247, in _map_single\n    batch = apply_function_on_filtered_inputs(\n  File \"c:\\Users\\Stephen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 3123, in apply_function_on_filtered_inputs\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n  File \"C:\\Users\\Stephen\\AppData\\Local\\Temp\\ipykernel_11336\\1974914204.py\", line 2, in tokenize_function\nNameError: name 'tokenizer' is not defined\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Stephen\\Documents\\University\\4th Year\\Capstone\\Iterations\\Advanced Language Model.ipynb Cell 57\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Stephen/Documents/University/4th%20Year/Capstone/Iterations/Advanced%20Language%20Model.ipynb#Y110sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Stephen/Documents/University/4th%20Year/Capstone/Iterations/Advanced%20Language%20Model.ipynb#Y110sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         row[\u001b[39m'\u001b[39m\u001b[39mattr_value_en_sentence\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Stephen/Documents/University/4th%20Year/Capstone/Iterations/Advanced%20Language%20Model.ipynb#Y110sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Stephen/Documents/University/4th%20Year/Capstone/Iterations/Advanced%20Language%20Model.ipynb#Y110sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Stephen/Documents/University/4th%20Year/Capstone/Iterations/Advanced%20Language%20Model.ipynb#Y110sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         max_length\u001b[39m=\u001b[39mMAX_SEQ_LEN,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Stephen/Documents/University/4th%20Year/Capstone/Iterations/Advanced%20Language%20Model.ipynb#Y110sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Stephen/Documents/University/4th%20Year/Capstone/Iterations/Advanced%20Language%20Model.ipynb#Y110sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m column_names \u001b[39m=\u001b[39m train_dataset\u001b[39m.\u001b[39mcolumn_names\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Stephen/Documents/University/4th%20Year/Capstone/Iterations/Advanced%20Language%20Model.ipynb#Y110sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m train_dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Stephen/Documents/University/4th%20Year/Capstone/Iterations/Advanced%20Language%20Model.ipynb#Y110sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     tokenize_function,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Stephen/Documents/University/4th%20Year/Capstone/Iterations/Advanced%20Language%20Model.ipynb#Y110sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Stephen/Documents/University/4th%20Year/Capstone/Iterations/Advanced%20Language%20Model.ipynb#Y110sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     num_proc\u001b[39m=\u001b[39;49mmultiprocessing\u001b[39m.\u001b[39;49mcpu_count(),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Stephen/Documents/University/4th%20Year/Capstone/Iterations/Advanced%20Language%20Model.ipynb#Y110sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     remove_columns\u001b[39m=\u001b[39;49mcolumn_names,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Stephen/Documents/University/4th%20Year/Capstone/Iterations/Advanced%20Language%20Model.ipynb#Y110sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Stephen/Documents/University/4th%20Year/Capstone/Iterations/Advanced%20Language%20Model.ipynb#Y110sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m valid_dataset \u001b[39m=\u001b[39m valid_dataset\u001b[39m.\u001b[39mmap(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Stephen/Documents/University/4th%20Year/Capstone/Iterations/Advanced%20Language%20Model.ipynb#Y110sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     tokenize_function,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Stephen/Documents/University/4th%20Year/Capstone/Iterations/Advanced%20Language%20Model.ipynb#Y110sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     batched\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Stephen/Documents/University/4th%20Year/Capstone/Iterations/Advanced%20Language%20Model.ipynb#Y110sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     num_proc\u001b[39m=\u001b[39mmultiprocessing\u001b[39m.\u001b[39mcpu_count(),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Stephen/Documents/University/4th%20Year/Capstone/Iterations/Advanced%20Language%20Model.ipynb#Y110sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     remove_columns\u001b[39m=\u001b[39mcolumn_names,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Stephen/Documents/University/4th%20Year/Capstone/Iterations/Advanced%20Language%20Model.ipynb#Y110sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Stephen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\arrow_dataset.py:2939\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   2934\u001b[0m         \u001b[39massert\u001b[39;00m (\n\u001b[0;32m   2935\u001b[0m             \u001b[39mlen\u001b[39m(results) \u001b[39m==\u001b[39m nb_of_missing_shards\n\u001b[0;32m   2936\u001b[0m         ), \u001b[39m\"\u001b[39m\u001b[39mThe number of missing cached shards needs to correspond to the number of `_map_single` we\u001b[39m\u001b[39m'\u001b[39m\u001b[39mre running\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2938\u001b[0m         \u001b[39mfor\u001b[39;00m index, async_result \u001b[39min\u001b[39;00m results\u001b[39m.\u001b[39mitems():\n\u001b[1;32m-> 2939\u001b[0m             transformed_shards[index] \u001b[39m=\u001b[39m async_result\u001b[39m.\u001b[39;49mget()\n\u001b[0;32m   2941\u001b[0m \u001b[39massert\u001b[39;00m (\n\u001b[0;32m   2942\u001b[0m     transformed_shards\u001b[39m.\u001b[39mcount(\u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m   2943\u001b[0m ), \u001b[39m\"\u001b[39m\u001b[39mAll shards have to be defined Datasets, none should still be missing.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2945\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConcatenating \u001b[39m\u001b[39m{\u001b[39;00mnum_proc\u001b[39m}\u001b[39;00m\u001b[39m shards\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Stephen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\multiprocess\\pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    772\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n\u001b[0;32m    773\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "def tokenize_function(row):\n",
    "    return tokenizer(\n",
    "        row['attr_value_en_sentence'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LEN,\n",
    "        return_special_tokens_mask=True)\n",
    "  \n",
    "column_names = train_dataset.column_names\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=multiprocessing.cpu_count(),\n",
    "    remove_columns=column_names,\n",
    ")\n",
    "\n",
    "valid_dataset = valid_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=multiprocessing.cpu_count(),\n",
    "    remove_columns=column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: attr_value_en_sentence, __index_level_0__. If attr_value_en_sentence, __index_level_0__ are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "c:\\Users\\Stephen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 0\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 33476\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "\n",
    "steps_per_epoch = int(len(train_dataset) / TRAIN_BATCH_SIZE)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./bert-news',\n",
    "    logging_dir='./LMlogs',             \n",
    "    num_train_epochs=2,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    warmup_steps=LR_WARMUP_STEPS,\n",
    "    save_steps=steps_per_epoch,\n",
    "    save_total_limit=3,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    learning_rate=LEARNING_RATE, \n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='loss', \n",
    "    greater_is_better=False,\n",
    "    seed=SEED_TRAIN\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"embeddings/\") #save your custom model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 2\n",
    "\n",
    "Followed this tutorial:\n",
    "https://towardsdatascience.com/masked-language-modelling-with-bert-7d49793e5d2c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting GPUtil\n",
      "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Installing collected packages: GPUtil\n",
      "  Running setup.py install for GPUtil: started\n",
      "  Running setup.py install for GPUtil: finished with status 'done'\n",
      "Successfully installed GPUtil-1.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: GPUtil is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0\n",
      "[notice] To update, run: C:\\Users\\Jihoon.DESKTOP-1HIBMQO\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install GPUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization — tokenization is simple, we’ve already initialized a BertTokenizer, all we do now is tokenize our input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "dtf_mlm = df_detailed[:500]\n",
    "\n",
    "# Train/Valid Split\n",
    "df_train, df_valid = train_test_split(\n",
    "    dtf_mlm, test_size=0.15, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(df_train['attr_value_en_sentence'].tolist(), return_tensors='pt', max_length=512, truncation=True, padding='max_length')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create labels — The next step is easy, all we need to do here is clone our input_ids tensor into a new labels tensor. We’ll store this within the inputs variable too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['labels'] = inputs.input_ids.detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Masking — Now we need to mask a random selection of tokens in our input_ids tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random array of floats with equal dimensions to input_ids tensor\n",
    "rand = torch.rand(inputs.input_ids.shape)\n",
    "# create mask array\n",
    "mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * \\\n",
    "           (inputs.input_ids != 102) * (inputs.input_ids != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ..., False, False, False],\n",
       "        [False,  True, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        ...,\n",
       "        [False,  True, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_arr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we take take the indices of each True value, within each individual vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = []\n",
    "\n",
    "for i in range(inputs.input_ids.shape[0]):\n",
    "    selection.append(\n",
    "        torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Getting the data ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductDetailDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ProductDetailDataset(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  5% |  9% |\n"
     ]
    }
   ],
   "source": [
    "gpu_usage()       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncuda.select_device(0)\\ncuda.close()\\ncuda.select_device(0)\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "cuda.select_device(0)\n",
    "cuda.close()\n",
    "cuda.select_device(0)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid argument\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [51], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[39m# and move our model over to the selected device\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m model\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m      4\u001b[0m \u001b[39m# activate training mode\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\modeling_utils.py:1682\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1677\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1678\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`.to` is not supported for `8-bit` models. Please use the model as it is, since the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1679\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1680\u001b[0m     )\n\u001b[0;32m   1681\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1682\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mto(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:907\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    903\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    904\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m    905\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m--> 907\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    577\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 578\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    580\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    581\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    582\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    583\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    589\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    577\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 578\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    580\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    581\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    582\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    583\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    589\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    577\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 578\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    580\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    581\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    582\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    583\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    589\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:601\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    597\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 601\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[0;32m    602\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    603\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:905\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    902\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[0;32m    903\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    904\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m--> 905\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: invalid argument\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# and move our model over to the selected device\n",
    "model.to(device)\n",
    "# activate training mode\n",
    "model.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Calculate Loss — Our final step here no different from the typical model training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize optimizer\n",
    "optim = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/27 [00:00<?, ?it/s]C:\\Users\\Jihoon.DESKTOP-1HIBMQO\\AppData\\Local\\Temp\\ipykernel_19480\\2002325694.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "  0%|          | 0/27 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid argument\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [33], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m optim\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     11\u001b[0m \u001b[39m# pull all tensor batches required for training\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m input_ids \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m     13\u001b[0m attention_mask \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m labels \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: invalid argument\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # for our progress bar\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # setup loop with TQDM and dataloader\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for batch in loop:\n",
    "        # initialize calculated gradients (from prev step)\n",
    "        optim.zero_grad()\n",
    "        # pull all tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        # process\n",
    "        outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        # extract loss\n",
    "        loss = outputs.loss\n",
    "        # calculate loss for every parameter that needs grad update\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        # print relevant info to progress bar\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'embeddings/preliminary_nlp_model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = torch.load('embeddings/preliminary_nlp_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving in a form that the huggingface provided.\n",
    "fine_tuned_model.save_pretrained(\"embeddings/preliminary_nlp_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_st_model = BertForMaskedLM.from_pretrained('embeddings/preliminary_nlp_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_embeddings = {}\n",
    "for i, row in df_detailed_subset.iterrows():\n",
    "    # Generate an embedding for the entire product sentence\n",
    "    generated_embedding = fine_tuned_model.bert(**tokenizer(row[\"attr_value_en_sentence\"],return_tensors=\"pt\"))[0][:,0,:].squeeze(0)\n",
    "    generated_embeddings[row['ctr_product_num']] = generated_embedding\n",
    "embeddings_dict = pd.DataFrame.from_dict(generated_embeddings, orient='index')\n",
    "embeddings_dict.index.names = [\"ctr_product_num\"]\n",
    "\n",
    "embeddings_dict.to_csv(\"embeddings/custom_nlp_1k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_detailed_subset[\u001b[39m'\u001b[39m\u001b[39miteration_1\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_detailed_subset\u001b[39m.\u001b[39;49mattr_value_en_sentence\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: fine_tuned_model\u001b[39m.\u001b[39;49mbert(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtokenizer(x,return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m))[\u001b[39m0\u001b[39;49m][:,\u001b[39m0\u001b[39;49m,:]\u001b[39m.\u001b[39;49msqueeze(\u001b[39m0\u001b[39;49m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\apply.py:1105\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1102\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1104\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1105\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\apply.py:1156\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1154\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1155\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1156\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1157\u001b[0m             values,\n\u001b[0;32m   1158\u001b[0m             f,\n\u001b[0;32m   1159\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1160\u001b[0m         )\n\u001b[0;32m   1162\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1163\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1164\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1165\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\_libs\\lib.pyx:2918\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn [14], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_detailed_subset[\u001b[39m'\u001b[39m\u001b[39miteration_1\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_detailed_subset\u001b[39m.\u001b[39mattr_value_en_sentence\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: fine_tuned_model\u001b[39m.\u001b[39mbert(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtokenizer(x,return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m))[\u001b[39m0\u001b[39m][:,\u001b[39m0\u001b[39m,:]\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1021\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1012\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1014\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m   1015\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1016\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1019\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1020\u001b[0m )\n\u001b[1;32m-> 1021\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m   1022\u001b[0m     embedding_output,\n\u001b[0;32m   1023\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   1024\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1025\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1026\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   1027\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1028\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1029\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1030\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1031\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1032\u001b[0m )\n\u001b[0;32m   1033\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1034\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\bert\\modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    601\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    603\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    608\u001b[0m     )\n\u001b[0;32m    609\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 610\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    611\u001b[0m         hidden_states,\n\u001b[0;32m    612\u001b[0m         attention_mask,\n\u001b[0;32m    613\u001b[0m         layer_head_mask,\n\u001b[0;32m    614\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    615\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    616\u001b[0m         past_key_value,\n\u001b[0;32m    617\u001b[0m         output_attentions,\n\u001b[0;32m    618\u001b[0m     )\n\u001b[0;32m    620\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    621\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\bert\\modeling_bert.py:496\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    485\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    486\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    493\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    494\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    495\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 496\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    497\u001b[0m         hidden_states,\n\u001b[0;32m    498\u001b[0m         attention_mask,\n\u001b[0;32m    499\u001b[0m         head_mask,\n\u001b[0;32m    500\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    501\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    502\u001b[0m     )\n\u001b[0;32m    503\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    505\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\bert\\modeling_bert.py:426\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    417\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    418\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    424\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    425\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 426\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    427\u001b[0m         hidden_states,\n\u001b[0;32m    428\u001b[0m         attention_mask,\n\u001b[0;32m    429\u001b[0m         head_mask,\n\u001b[0;32m    430\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    431\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    432\u001b[0m         past_key_value,\n\u001b[0;32m    433\u001b[0m         output_attentions,\n\u001b[0;32m    434\u001b[0m     )\n\u001b[0;32m    435\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[0;32m    436\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\bert\\modeling_bert.py:307\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    305\u001b[0m     value_layer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([past_key_value[\u001b[39m1\u001b[39m], value_layer], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m    306\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 307\u001b[0m     key_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey(hidden_states))\n\u001b[0;32m    308\u001b[0m     value_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue(hidden_states))\n\u001b[0;32m    310\u001b[0m query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_detailed_subset['iteration_1'] = df_detailed_subset.attr_value_en_sentence.apply(lambda x: fine_tuned_model.bert(**tokenizer(x,return_tensors=\"pt\"))[0][:,0,:].squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_detailed_subset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train using wandbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='out',\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=2\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "#os.environ[\"WANDB_MODE\"] = \"online\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "39b28e74a53c7fe6627b99d166fc22c7c646315a2e88d31074bfa870e9a41665"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
