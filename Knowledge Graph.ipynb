{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Graph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a knowledge graph, different relationships between products and stores are extracted from the CTC datasets. The current focus is to create a dense representation of the data with a high quality knowledge graph. The graph will later be converted into embeddings. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jihoon.DESKTOP-1HIBMQO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torchkge.data_structures import KnowledgeGraph\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.handlers import EarlyStopping\n",
    "from ignite.metrics import RunningAverage\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torchkge.evaluation import LinkPredictionEvaluator\n",
    "from torchkge.models import TransRModel\n",
    "from torchkge.models import TransEModel\n",
    "from torchkge.sampling import BernoulliNegativeSampler\n",
    "from torchkge.utils import MarginLoss, DataLoader\n",
    "from torchkge.utils.datasets import load_fb15k\n",
    "from torchkge.data_structures import KnowledgeGraph\n",
    "from tqdm.autonotebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basket_path = \"clean_data/cleaned_basket.csv\"\n",
    "df_basket = pd.read_csv(basket_path).drop(columns=['Unnamed: 0'])\n",
    "df_basket[\"product_num\"].values.astype(int, copy=False)\n",
    "df_basket[\"store_num\"].values.astype(int, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_path = \"clean_data/cleaned_products.csv\"\n",
    "df_products = pd.read_csv(products_path).drop(columns=['Unnamed: 0'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources and Relevant Links"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/nageshsingh/build-knowledge-graph-using-python\n",
    "\n",
    "https://aws-dglke.readthedocs.io/en/latest/kg.html \n",
    "\n",
    "https://arxiv.org/abs/2107.07842 \n",
    "\n",
    "Idea:\n",
    "- Nodes: products, stores, and hierarchical categories \n",
    "- Relationships (Edges): \"bought together\" (using basket data), \"sold in\" (store data), \"part of\"/\"subcategory of\" (product hierarchical data) \n",
    "\n",
    "\n",
    "Product Knowledge Graph Embedding for E-commerce: https://arxiv.org/pdf/1911.12481.pdf \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Extract Relationships for the Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bought_together_pairs(df,groupby,target_column):\n",
    "    grouped_df = df.groupby(groupby)\n",
    "    all_pairs = [] #source,target\n",
    "\n",
    "    for key, item in grouped_df:\n",
    "        groups = item[target_column].unique()\n",
    "        if len(groups)>1:\n",
    "            for index, source in enumerate(groups):\n",
    "                for target in groups[index+1:]:\n",
    "                    all_pairs.append([source,target, \"bought together\"])\n",
    "    return all_pairs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract relationships from the Basket Data "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basket data allows us to see which products have been bought together. Thus, we can extract relationships of the form '[product A, product B, \"bought together]'. The triplets are stored in the list called bought_together. Extracting these relationships could take some time, so the data has been pickled so the extraction does not need to occur every time the notebook is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify \"product\" in front of product nums and \"store\" in front of store nums for clarity\n",
    "df_basket['product_num'] = df_basket['product_num'].apply(lambda x: \"{}{}\".format('product ', x))\n",
    "df_basket['store_num'] = df_basket['store_num'].apply(lambda x: \"{}{}\".format('store ', x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basket"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the \"bought together\" relationship from the basket data takes a while to run. We created a pickle data file to load instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbought_together = bought_together_pairs(df_basket,\"basket_id\",\"product_num\") #source,target\\nlen(bought_together)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "bought_together = bought_together_pairs(df_basket,\"basket_id\",\"product_num\") #source,target\n",
    "len(bought_together)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#pickle the data\\nwith open('embeddings/pickle_bought_together.data', 'wb') as f:\\n        pickle.dump(bought_together, f)\\n\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "#pickle the data\n",
    "with open('embeddings/pickle_bought_together.data', 'wb') as f:\n",
    "        pickle.dump(bought_together, f)\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the pickled data and save\n",
    "infile = open('embeddings/pickle_bought_together.data','rb')\n",
    "bought_together = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract relationships from Product Standard Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The product standard dataset will be filtered down to the products which appear in the basket data. Three relationships will be extracted to capture products that are part of categories, and categories that are subcategories of larger ones.\n",
    "\n",
    "[ctr_product_num, merch_bus_cat_nm, \"part of\"]\n",
    "\n",
    "[merch_bus_cat_nm, merch_lob_nm, \"subcategory of\"] \n",
    "\n",
    "[merch_lob_nm, merch_division_nm, \"subcategory of\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the word \"product\" before product num (differentiate from store nums later on)\n",
    "df_products['ctr_product_num'] = df_products['ctr_product_num'].apply(lambda x: \"{}{}\".format('product ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter products to those that appear in basket data\n",
    "df_products_basket = df_products[df_products.ctr_product_num.isin(df_basket.product_num.unique())]\n",
    "len(df_products_basket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products_basket"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified the get_pairs function to include the relation that is being captured as an input parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs_relation(df,groupby,target_column,relation):\n",
    "    grouped_df = df.groupby(groupby)\n",
    "    all_pairs = [] #source,target\n",
    "\n",
    "    for source, item in grouped_df:\n",
    "        groups = item[target_column].unique()\n",
    "        for index, target in enumerate(groups):\n",
    "            all_pairs.append([source, target, relation])\n",
    "    return all_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[product num, business category, relation: \"part of\"]\n",
    "prod_buscat = get_pairs_relation(df_products_basket, \"ctr_product_num\", \"merch_bus_cat_nm\", \"part of\")\n",
    "prod_buscat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[merch bus cat, merch lob nm, relation: \"subcategory of\"]\n",
    "buscat_lob = get_pairs_relation(df_products_basket, \"merch_bus_cat_nm\", \"merch_lob_nm\", \"subcategory of\")\n",
    "buscat_lob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lob_division = get_pairs_relation(df_products_basket, \"merch_lob_nm\", \"merch_division_nm\", \"subcategory of\")\n",
    "lob_division"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract store semantics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_store = get_pairs_relation(df_basket,\"product_num\", \"store_num\", \"sold at\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_store"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Construct the Knowledge Graph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all extracted relationships and put them into a dataframe for the KG. It must have three columns: \"from\", \"to\", and \"rel\" to describe pairs of nodes and their relationships.\n",
    "\n",
    "Note: currently, the product and business category relation (prod_buscat) causes a strange error, so it is excluded from this version of the knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine the extracted relationships into one list\n",
    "all_rels = bought_together  + buscat_lob + lob_division + in_store # prod_buscat\n",
    "len(all_rels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract subject\n",
    "source = [i[0] for i in all_rels] \n",
    "\n",
    "# extract object\n",
    "target = [i[1] for i in all_rels] \n",
    "\n",
    "# state relationship\n",
    "relations = [i[2] for i in all_rels] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign to dataframe\n",
    "kg_df = pd.DataFrame({'from':source, 'to':target, 'rel':relations})\n",
    "kg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the df into train val and test \n",
    "df_train, df_val, df_test = np.split(kg_df.sample(frac=1, random_state=42), [int(.6*len(kg_df)), int(.8*len(kg_df))])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the knowledge graph takes about 32 minutes. The train, val and test kgs have been pickeld for easier future use. If you wish to redo the KG generation and pickling, uncomment the two cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Turn into knowledge graph\n",
    "kg_train = KnowledgeGraph(df=df_train)\n",
    "kg_val = KnowledgeGraph(df=df_val)\n",
    "kg_test = KnowledgeGraph(df=df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#pickle the train, val, and test KGs\n",
    "with open('embeddings/pickle_kg_train.data', 'wb') as f:\n",
    "        pickle.dump(kg_train, f)\n",
    "\n",
    "with open('embeddings/pickle_kg_val.data', 'wb') as f:\n",
    "        pickle.dump(kg_val, f)\n",
    "\n",
    "with open('embeddings/pickle_kg_test.data', 'wb') as f:\n",
    "        pickle.dump(kg_test, f)\n",
    "\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pickles could take a few minutes to open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the pickles\n",
    "infile = open('embeddings/pickle_kg_train.data','rb')\n",
    "kg_train = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "infile = open('embeddings/pickle_kg_val.data','rb')\n",
    "kg_val = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "infile = open('embeddings/pickle_kg_test.data','rb')\n",
    "kg_test = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create Knowledge Graph Embedding using torchkge\n",
    "\n",
    "https://torchkge.readthedocs.io/en/latest/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with Ignite, following https://torchkge.readthedocs.io/en/latest/tutorials/training.html\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://kge-tutorial-ecai2020.github.io/ECAI-20_KGE_tutorial.pdf\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conversion of knowledge graphs into embeddings is still in progress. The intuition is to choose a translational model and attempt link prediction between the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def process_batch(engine, batch):\n",
    "    h, t, r = batch[0], batch[1], batch[2]\n",
    "    n_h, n_t = sampler.corrupt_batch(h, t, r)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    pos, neg = model(h, t, r, n_h, n_t)\n",
    "    loss = criterion(pos, neg)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def linkprediction_evaluation(engine):\n",
    "    model.normalize_parameters()\n",
    "\n",
    "    loss = engine.state.output\n",
    "\n",
    "    # validation MRR measure\n",
    "    if engine.state.epoch % eval_epoch == 0:\n",
    "        evaluator = LinkPredictionEvaluator(model, kg_val)\n",
    "        evaluator.evaluate(b_size=256, verbose=False)\n",
    "        val_mrr = evaluator.mrr()[1]\n",
    "    else:\n",
    "        val_mrr = 0\n",
    "\n",
    "    print('Epoch {} | Train loss: {}, Validation MRR: {}'.format(\n",
    "        engine.state.epoch, loss, val_mrr))\n",
    "\n",
    "    try:\n",
    "        if engine.state.best_mrr < val_mrr:\n",
    "            engine.state.best_mrr = val_mrr\n",
    "        return val_mrr\n",
    "\n",
    "    except AttributeError as e:\n",
    "        if engine.state.epoch == 1:\n",
    "            engine.state.best_mrr = val_mrr\n",
    "            return val_mrr\n",
    "        else:\n",
    "            raise e\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "infile = open('embeddings/pickle_bought_together.data','rb')\n",
    "bought_together_pairs = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "# extract subject\n",
    "source = [i[0] for i in bought_together_pairs]\n",
    "\n",
    "# extract object\n",
    "target = [i[1] for i in bought_together_pairs]\n",
    "\n",
    "# state relationship\n",
    "relations = [\"bought together\" for i in bought_together_pairs]\n",
    "\n",
    "kg_df = pd.DataFrame({'from':source, 'to':target, 'rel':relations})\n",
    "\n",
    "df_train, df_val, df_test = np.split(kg_df.sample(frac=1, random_state=42), [int(.6*len(kg_df)), int(.8*len(kg_df))])\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 0: Bought Together KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn into knowledge graph\n",
    "#kg_train = KnowledgeGraph(df=df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kg_val = KnowledgeGraph(df=df_val)\n",
    "#kg_test = KnowledgeGraph(df=df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('embeddings/pickle_merch_pairs_kg_train.data', 'wb') as f:\n",
    "#        pickle.dump(kg_train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('embeddings/pickle_merch_pairs_kg_val.data', 'wb') as f:\n",
    "#        pickle.dump(kg_val, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('embeddings/pickle_merch_pairs_kg_test.data', 'wb') as f:\n",
    "#        pickle.dump(kg_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ninfile = open('embeddings/pickle_merch_pairs_kg_train.data','rb')\\nkg_train = pickle.load(infile)\\ninfile.close()\\n\\ninfile = open('embeddings/pickle_merch_pairs_kg_val.data','rb')\\nkg_val = pickle.load(infile)\\ninfile.close()\\n\\ninfile = open('embeddings/pickle_merch_pairs_kg_test.data','rb')\\nkg_test = pickle.load(infile)\\ninfile.close()\\n\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "infile = open('embeddings/pickle_merch_pairs_kg_train.data','rb')\n",
    "kg_train = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "infile = open('embeddings/pickle_merch_pairs_kg_val.data','rb')\n",
    "kg_val = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "infile = open('embeddings/pickle_merch_pairs_kg_test.data','rb')\n",
    "kg_test = pickle.load(infile)\n",
    "infile.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\n\\n# prevent memory issue\\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \\'max_split_size_mb:512\\'\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import os\n",
    "\n",
    "# prevent memory issue\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = 'max_split_size_mb:512'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ent_emb_dim = 1000\n",
    "rel_emb_dim = 3\n",
    "lr = 0.0005\n",
    "n_epochs = 100\n",
    "b_size = 32768\n",
    "margin = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model and criterion\n",
    "model = TransRModel(ent_emb_dim,rel_emb_dim, kg_train.n_ent, kg_train.n_rel)\n",
    "criterion = MarginLoss(margin)\n",
    "\n",
    "# Move everything to CUDA if available\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    model.cuda()\n",
    "    criterion.cuda()\n",
    "\n",
    "# Define the torch optimizer to be used\n",
    "optimizer = Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "sampler = BernoulliNegativeSampler(kg_train)\n",
    "dataloader = DataLoader(kg_train, batch_size=b_size, use_cuda='all')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100 | mean loss: 2436.30601: 100%|██████████| 100/100 [4:38:47<00:00, 167.27s/epoch]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "iterator = tqdm(range(n_epochs), unit='epoch')\n",
    "for epoch in iterator:\n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        h, t, r = batch[0], batch[1], batch[2]\n",
    "        n_h, n_t = sampler.corrupt_batch(h, t, r)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        pos, neg = model(h, t, r, n_h, n_t)\n",
    "        loss = criterion(pos, neg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    iterator.set_description(\n",
    "        'Epoch {} | mean loss: {:.5f}'.format(epoch + 1,\n",
    "                                              running_loss / len(dataloader)))\n",
    "\n",
    "model.normalize_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"embeddings/transr_state_dict_version_2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransRModel(\n",
       "  (ent_emb): Embedding(143618, 1000)\n",
       "  (rel_emb): Embedding(3, 3)\n",
       "  (proj_mat): Embedding(3, 3000)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransRModel(\n",
       "  (ent_emb): Embedding(143618, 1000)\n",
       "  (rel_emb): Embedding(3, 3)\n",
       "  (proj_mat): Embedding(3, 3000)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransRModel(ent_emb_dim,rel_emb_dim, kg_train.n_ent, kg_train.n_rel)  \n",
    "model.load_state_dict(torch.load(\"embeddings/transr_state_dict_version_2.pt\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_emb,rel_emb, proj_mat = model.get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_train.evaluate_dicts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "419235e4e32b5f4d857e46aa602574303c278266de811801684b7022f6c1e83a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
