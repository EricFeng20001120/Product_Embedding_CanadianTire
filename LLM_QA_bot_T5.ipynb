{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Language Model Question and Answering Bot.\n",
    "\n",
    "This will be based on the sentenceT5 by google \n",
    "\n",
    "Reference: https://github.com/amrrs/LLM-QA-Bot/blob/main/LLM_Q%26A_with_Open_Source_Hugging_Face_Models.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 16 20:42:04 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 531.18                 Driver Version: 531.18       CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3070 Ti    WDDM | 00000000:2B:00.0  On |                  N/A |\n",
      "| 39%   59C    P0               72W / 290W|    860MiB /  8192MiB |      2%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A       248    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A      1268    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A      6836    C+G   ...al\\Discord\\app-1.0.9011\\Discord.exe    N/A      |\n",
      "|    0   N/A  N/A      8928    C+G   ...les\\Microsoft OneDrive\\OneDrive.exe    N/A      |\n",
      "|    0   N/A  N/A     10036    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     10364    C+G   ....0_x64__kzh8wxbdkxb8p\\DCv2\\DCv2.exe    N/A      |\n",
      "|    0   N/A  N/A     13028    C+G   ...\\cef\\cef.win7x64\\steamwebhelper.exe    N/A      |\n",
      "|    0   N/A  N/A     13076    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A     14204    C+G   ...les\\Elgato\\CameraHub\\Camera Hub.exe    N/A      |\n",
      "|    0   N/A  N/A     15496    C+G   ...oogle\\Chrome\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     17072    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     17396    C+G   ...Programs\\Microsoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A     17928    C+G   ...Desktop\\app-3.2.0\\GitHubDesktop.exe    N/A      |\n",
      "|    0   N/A  N/A     19660    C+G   ...siveControlPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     24112    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyproject.toml in c:\\users\\jihoon.desktop-1hibmqo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.0.10)\n",
      "Requirement already satisfied: wheel in c:\\users\\jihoon.desktop-1hibmqo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pyproject.toml) (0.40.0)\n",
      "Requirement already satisfied: toml in c:\\users\\jihoon.desktop-1hibmqo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pyproject.toml) (0.10.2)\n",
      "Requirement already satisfied: jsonschema in c:\\users\\jihoon.desktop-1hibmqo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pyproject.toml) (4.17.3)\n",
      "Requirement already satisfied: setuptools>=42 in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.10_3.10.2800.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from pyproject.toml) (65.5.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\jihoon.desktop-1hibmqo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jsonschema->pyproject.toml) (0.19.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\jihoon.desktop-1hibmqo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jsonschema->pyproject.toml) (22.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: C:\\Users\\Jihoon.DESKTOP-1HIBMQO\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: C:\\Users\\Jihoon.DESKTOP-1HIBMQO\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install pyproject.toml\n",
    "! pip install -q langchain gpt-index llama-index transformers sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jihoon.DESKTOP-1HIBMQO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index import SimpleDirectoryReader, LangchainEmbedding, GPTListIndex,GPTSimpleVectorIndex, PromptHelper\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index import LLMPredictor\n",
    "import torch\n",
    "from langchain.llms.base import LLM\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlanLLM(LLM):\n",
    "    model_name = \"google/flan-t5-base\"\n",
    "    pipeline = pipeline(\"text2text-generation\", model=model_name, device=0, model_kwargs={\"torch_dtype\":torch.bfloat16})\n",
    "\n",
    "    def _call(self, prompt, stop=None):\n",
    "        return self.pipeline(prompt, max_length=9999)[0][\"generated_text\"]\n",
    " \n",
    "    def _identifying_params(self):\n",
    "        return {\"name_of_model\": self.model_name}\n",
    "\n",
    "    def _llm_type(self):\n",
    "        return \"custom\"\n",
    "\n",
    "\n",
    "llm_predictor = LLMPredictor(llm=FlanLLM())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "hfemb = HuggingFaceEmbeddings()\n",
    "embed_model = LangchainEmbedding(hfemb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import CTC product data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from llama_index import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_detail_detail_path = \"clean_data/cleaned_products_detailed.csv\"\n",
    "df_product_standard = pd.read_csv(product_detail_detail_path)\n",
    "df_detailed = df_product_standard[['ctr_product_num','attr_value_en_sentence']]\n",
    "df_detailed = df_detailed.dropna()\n",
    "df_detailed = df_detailed.drop_duplicates()\n",
    "df_detailed_subset = df_detailed.sample(frac=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jihoon.DESKTOP-1HIBMQO\\AppData\\Local\\Temp\\ipykernel_23432\\271546728.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_right['ctr_product_num'] = pd.to_numeric(df_right['ctr_product_num'], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "path_standard = \"Data\\product\\product_standard_attributes.csv\" \n",
    "\n",
    "# put it in data farme\n",
    "df_product_standard = pd.read_csv(path_standard, low_memory=False) #lazyway to solve low memory issue\n",
    "df_right =df_product_standard[['ctr_product_num','merch_lob_nm','package_depth_qty','short_desc','package_height_qty','package_width_qty','package_volume_qty','package_weight_qty']]\n",
    "\n",
    "df_right['ctr_product_num'] = pd.to_numeric(df_right['ctr_product_num'], errors='coerce')\n",
    "df_right = df_right.dropna(subset=['ctr_product_num'])\n",
    "df_right['ctr_product_num'] = df_right['ctr_product_num'].astype('int')\n",
    "df_detailed_subset = df_detailed_subset.merge(df_right, on='ctr_product_num', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_detailed_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_right =df_product_standard[['ctr_product_num','merch_lob_nm','package_depth_qty','short_desc','package_height_qty','package_width_qty','package_volume_qty','package_weight_qty']]\n",
    "\n",
    "\n",
    "docLabels = []\n",
    "# getting all the names of all json files\n",
    "docLabels = [f for f in df_detailed_subset['package_depth_qty']]\n",
    "\n",
    "sdecLabels = []\n",
    "# getting all the names of all json files\n",
    "sdecLabels = [f for f in df_detailed_subset['short_desc']]\n",
    "\n",
    "subCatLabels = []\n",
    "# getting all the names of all json files\n",
    "subCatLabels = [f for f in df_detailed_subset['package_height_qty']]\n",
    "widthLabels = [f for f in df_detailed_subset['package_width_qty']]\n",
    "volumnLabels = [f for f in df_detailed_subset['package_volume_qty']]\n",
    "weightLabels = [f for f in df_detailed_subset['package_weight_qty']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "j = 0\n",
    "for i, row in df_detailed_subset.iterrows():\n",
    "    #print(row['attr_value_en_sentence'])\n",
    "    #data.append(row['attr_value_en_sentence'].split())\n",
    "    output = 'package depth is ' + str(docLabels[j]) + '. ' + 'name of the product is ' + str(sdecLabels[j]) + '. ' + 'package height is ' + str(subCatLabels[j]) + '. '+'package width is ' + str(widthLabels[j]) + '. '+'package volumn is ' + str(volumnLabels[j]) + '. ' +'package weight is ' + str(weightLabels[j]) + '. ' + 'full description is ' + str(row['attr_value_en_sentence'] + '. ')\n",
    "    data.append(output)\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [Document(t) for t in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_documents = documents[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of output tokens\n",
    "num_output = 250\n",
    "# set maximum input size\n",
    "max_input_size = 512\n",
    "# set maximum chunk overlap\n",
    "max_chunk_overlap = 20\n",
    "\n",
    "\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index = GPTSimpleVectorIndex(documents, embed_model=embed_model, llm_predictor=llm_predictor)\n",
    "\n",
    "index = GPTListIndex(test_documents, embed_model=embed_model, llm_predictor=llm_predictor)\n",
    "\n",
    "#index.save_to_disk('index.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response = index.query(\"product that is good for indoor\") \n",
    "#print(response) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TT+ ---------\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"how can be categorize the CLASS 2 HTCH 1-1/4?\") \n",
    "print(response) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jihoon.DESKTOP-1HIBMQO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\pipelines\\base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lights up and self inflates in secondsStakes and tethers includedPlugs in to standard outlet ------------\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"what is TT+?\") \n",
    "print(response) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cheery pick instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_documents[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = index.query(\"which product will be helpful for appearacbce if my indoor space?\") \n",
    "print(response) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = index.query(\"What kind of product is EC BOHEM BLK 6X9?\") \n",
    "print(response) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EC BOHEM BLK 6X9 is not recyclable\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"why is EC BOHEM BLK 6X9 non recyclable?\") \n",
    "print(response) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a raised lip to trap and contain spills, dirt, and grease away from your vehicle's interiorFeatures a textured surface to help prevent cargo from shiftingMade in Turkey of high quality viscosePerfect area rug for medium to high traffic areasBlack modern/contemporary abstract indoor area rugRug is soft underfoot / to touch. ------------\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"why is EC BOHEM BLK 6X9 is non recyclable?\") \n",
    "print(response) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
